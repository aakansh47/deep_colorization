{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of given Image List 750\n",
      "Number of images -  751\n",
      "Training Set Size :  675\n",
      "Test Set Size :  76\n",
      "Length of training Image List 674\n",
      "Length of testing Image List 76\n",
      "For Tanh Function\n",
      "Device: cuda:0\n",
      "Length of Augmented Dataset 6740\n",
      "Activation Function:  tanh\n",
      "..Colorizer Training started..\n",
      "epoch: 0, loss: 8.621428595739417\n",
      "epoch: 1, loss: 0.5397621112060733\n",
      "epoch: 2, loss: 0.35630962281720713\n",
      "epoch: 3, loss: 0.288689888373483\n",
      "epoch: 4, loss: 0.22759299777681008\n",
      "epoch: 5, loss: 0.19920429721241817\n",
      "epoch: 6, loss: 0.18773529079044238\n",
      "epoch: 7, loss: 0.1766558014205657\n",
      "epoch: 8, loss: 0.16603903219220228\n",
      "epoch: 9, loss: 0.16340830575791188\n",
      "epoch: 10, loss: 0.14910932462953497\n",
      "epoch: 11, loss: 0.1418149634700967\n",
      "epoch: 12, loss: 0.13657412499014754\n",
      "epoch: 13, loss: 0.12891608047357295\n",
      "epoch: 14, loss: 0.12473352342203725\n",
      "epoch: 15, loss: 0.1229697744565783\n",
      "epoch: 16, loss: 0.11733721422206145\n",
      "epoch: 17, loss: 0.11714104948623572\n",
      "epoch: 18, loss: 0.11586829379666597\n",
      "epoch: 19, loss: 0.1122999177314341\n",
      "epoch: 20, loss: 0.1119135492190253\n",
      "epoch: 21, loss: 0.1110113536415156\n",
      "epoch: 22, loss: 0.10756772181775887\n",
      "epoch: 23, loss: 0.10626703609887045\n",
      "epoch: 24, loss: 0.10459417644597124\n",
      "epoch: 25, loss: 0.1036654834751971\n",
      "epoch: 26, loss: 0.09936549430130981\n",
      "epoch: 27, loss: 0.10066987021127716\n",
      "epoch: 28, loss: 0.0974592944548931\n",
      "epoch: 29, loss: 0.09538295418315101\n",
      "epoch: 30, loss: 0.09470277070067823\n",
      "epoch: 31, loss: 0.09265485193463974\n",
      "epoch: 32, loss: 0.09161896689329296\n",
      "epoch: 33, loss: 0.08828149644250516\n",
      "epoch: 34, loss: 0.08764571307983715\n",
      "epoch: 35, loss: 0.08651471910707187\n",
      "epoch: 36, loss: 0.08641510565939825\n",
      "epoch: 37, loss: 0.08269952355476562\n",
      "epoch: 38, loss: 0.0827203584703966\n",
      "epoch: 39, loss: 0.08119050864479505\n",
      "epoch: 40, loss: 0.08195127724320628\n",
      "epoch: 41, loss: 0.07791732122132089\n",
      "epoch: 42, loss: 0.08005197324382607\n",
      "epoch: 43, loss: 0.07846365496516228\n",
      "epoch: 44, loss: 0.07607601079507731\n",
      "epoch: 45, loss: 0.07676988409366459\n",
      "epoch: 46, loss: 0.07703074676101096\n",
      "epoch: 47, loss: 0.07520998582913307\n",
      "epoch: 48, loss: 0.07794077046128223\n",
      "epoch: 49, loss: 0.0762454297291697\n",
      "epoch: 50, loss: 0.07500362821156159\n",
      "epoch: 51, loss: 0.07410634792177007\n",
      "epoch: 52, loss: 0.07523832162405597\n",
      "epoch: 53, loss: 0.07351394578290638\n",
      "epoch: 54, loss: 0.07314248391776346\n",
      "epoch: 55, loss: 0.07295271978364326\n",
      "epoch: 56, loss: 0.07226500452816254\n",
      "epoch: 57, loss: 0.072331668343395\n",
      "epoch: 58, loss: 0.0695175407017814\n",
      "epoch: 59, loss: 0.07183349997649202\n",
      "epoch: 60, loss: 0.07242526371555869\n",
      "epoch: 61, loss: 0.07056049013772281\n",
      "epoch: 62, loss: 0.07139523222576827\n",
      "epoch: 63, loss: 0.07072829274693504\n",
      "epoch: 64, loss: 0.07010028587683337\n",
      "epoch: 65, loss: 0.06821265811595367\n",
      "epoch: 66, loss: 0.06954451296769548\n",
      "epoch: 67, loss: 0.06959072753670625\n",
      "epoch: 68, loss: 0.06863143826922169\n",
      "epoch: 69, loss: 0.06870231126231374\n",
      "epoch: 70, loss: 0.06669513236556668\n",
      "epoch: 71, loss: 0.06965044455137104\n",
      "epoch: 72, loss: 0.06866609735880047\n",
      "epoch: 73, loss: 0.06897587287676288\n",
      "epoch: 74, loss: 0.0663624943481409\n",
      "epoch: 75, loss: 0.0684785404446302\n",
      "epoch: 76, loss: 0.06776783768145833\n",
      "epoch: 77, loss: 0.06717241672595264\n",
      "epoch: 78, loss: 0.06675328169512795\n",
      "epoch: 79, loss: 0.06612068620597711\n",
      "epoch: 80, loss: 0.06584419526916463\n",
      "epoch: 81, loss: 0.066410044892109\n",
      "epoch: 82, loss: 0.06580447416490642\n",
      "epoch: 83, loss: 0.065848804282723\n",
      "epoch: 84, loss: 0.065057425024861\n",
      "epoch: 85, loss: 0.06496113158209482\n",
      "epoch: 86, loss: 0.0638853050841135\n",
      "epoch: 87, loss: 0.0659861241147155\n",
      "epoch: 88, loss: 0.06565640596818412\n",
      "epoch: 89, loss: 0.06392617827805225\n",
      "epoch: 90, loss: 0.06554728226183215\n",
      "epoch: 91, loss: 0.06473773139441619\n",
      "epoch: 92, loss: 0.0657048013235908\n",
      "epoch: 93, loss: 0.061853718165366445\n",
      "epoch: 94, loss: 0.06576679660793161\n",
      "epoch: 95, loss: 0.0646267633565003\n",
      "epoch: 96, loss: 0.06248548119037878\n",
      "epoch: 97, loss: 0.06319367656396935\n",
      "epoch: 98, loss: 0.06314220785861835\n",
      "epoch: 99, loss: 0.06267895147175295\n",
      "epoch: 100, loss: 0.06030515855673002\n",
      "epoch: 101, loss: 0.06378446560847806\n",
      "epoch: 102, loss: 0.06257216777157737\n",
      "epoch: 103, loss: 0.06399927366874181\n",
      "epoch: 104, loss: 0.06272092407743912\n",
      "epoch: 105, loss: 0.06244692330074031\n",
      "epoch: 106, loss: 0.06408852666936582\n",
      "epoch: 107, loss: 0.061004232047707774\n",
      "epoch: 108, loss: 0.060604650672758\n",
      "epoch: 109, loss: 0.06178509568417212\n",
      "epoch: 110, loss: 0.06283597257424844\n",
      "epoch: 111, loss: 0.061241436953423545\n",
      "epoch: 112, loss: 0.061028519085084554\n",
      "epoch: 113, loss: 0.06206344169913791\n",
      "epoch: 114, loss: 0.06227735937864054\n",
      "epoch: 115, loss: 0.060102438161266036\n",
      "epoch: 116, loss: 0.060552762617589906\n",
      "epoch: 117, loss: 0.061312519981584046\n",
      "epoch: 118, loss: 0.060674592772556935\n",
      "epoch: 119, loss: 0.0619284556232742\n",
      "epoch: 120, loss: 0.059722051526478026\n",
      "epoch: 121, loss: 0.06006668760528555\n",
      "epoch: 122, loss: 0.05970901673572371\n",
      "epoch: 123, loss: 0.059110003858222626\n",
      "epoch: 124, loss: 0.06163511565682711\n",
      "epoch: 125, loss: 0.06002050881215837\n",
      "epoch: 126, loss: 0.05936547387682367\n",
      "epoch: 127, loss: 0.06025253280677134\n",
      "epoch: 128, loss: 0.06134176213527098\n",
      "epoch: 129, loss: 0.0599167437030701\n",
      "epoch: 130, loss: 0.05945540466927923\n",
      "epoch: 131, loss: 0.05916350344341481\n",
      "epoch: 132, loss: 0.06110477693437133\n",
      "epoch: 133, loss: 0.058723243288113736\n",
      "epoch: 134, loss: 0.059996132098603994\n",
      "epoch: 135, loss: 0.06058895106980344\n",
      "epoch: 136, loss: 0.05986972273240099\n",
      "epoch: 137, loss: 0.05837047565728426\n",
      "epoch: 138, loss: 0.060145962597744074\n",
      "epoch: 139, loss: 0.06001618377194973\n",
      "epoch: 140, loss: 0.061156224059232045\n",
      "epoch: 141, loss: 0.06063726309366757\n",
      "epoch: 142, loss: 0.059352493335609324\n",
      "epoch: 143, loss: 0.059440378077852074\n",
      "epoch: 144, loss: 0.05829817532503512\n",
      "epoch: 145, loss: 0.057757671835133806\n",
      "epoch: 146, loss: 0.05907743253192166\n",
      "epoch: 147, loss: 0.05950761921121739\n",
      "epoch: 148, loss: 0.05927994020021288\n",
      "epoch: 149, loss: 0.05997727833891986\n",
      "epoch: 150, loss: 0.059565347895841114\n",
      "epoch: 151, loss: 0.059931243769824505\n",
      "epoch: 152, loss: 0.057612717821029946\n",
      "epoch: 153, loss: 0.0597418136021588\n",
      "epoch: 154, loss: 0.05859777544537792\n",
      "epoch: 155, loss: 0.058948563841113355\n",
      "epoch: 156, loss: 0.058085035809199326\n",
      "epoch: 157, loss: 0.05909336666809395\n",
      "epoch: 158, loss: 0.05734777927136747\n",
      "epoch: 159, loss: 0.05889501683122944\n",
      "epoch: 160, loss: 0.059383265404903796\n",
      "epoch: 161, loss: 0.05848476791288704\n",
      "epoch: 162, loss: 0.058269839762942865\n",
      "epoch: 163, loss: 0.059214837063336745\n",
      "epoch: 164, loss: 0.05751116477767937\n",
      "epoch: 165, loss: 0.05752342074265471\n",
      "epoch: 166, loss: 0.05968757743539754\n",
      "epoch: 167, loss: 0.05626618888345547\n",
      "epoch: 168, loss: 0.05849240495444974\n",
      "epoch: 169, loss: 0.0571876924004755\n",
      "epoch: 170, loss: 0.058178889128612354\n",
      "epoch: 171, loss: 0.05718415937735699\n",
      "epoch: 172, loss: 0.05543559946818277\n",
      "epoch: 173, loss: 0.058591032087861095\n",
      "epoch: 174, loss: 0.058187272945360746\n",
      "epoch: 175, loss: 0.05561159150238382\n",
      "epoch: 176, loss: 0.05950870846572798\n",
      "epoch: 177, loss: 0.05877655117365066\n",
      "epoch: 178, loss: 0.058403069706400856\n",
      "epoch: 179, loss: 0.05710915380041115\n",
      "epoch: 180, loss: 0.05720954744174378\n",
      "epoch: 181, loss: 0.057405831743380986\n",
      "epoch: 182, loss: 0.0572387375505059\n",
      "epoch: 183, loss: 0.057976541080279276\n",
      "epoch: 184, loss: 0.05927675795828691\n",
      "epoch: 185, loss: 0.057741266369703226\n",
      "epoch: 186, loss: 0.058065650970092975\n",
      "epoch: 187, loss: 0.05662153761659283\n",
      "epoch: 188, loss: 0.05694023132673465\n",
      "epoch: 189, loss: 0.05903392357140547\n",
      "epoch: 190, loss: 0.05661360669910209\n",
      "epoch: 191, loss: 0.05718683326995233\n",
      "epoch: 192, loss: 0.05846455687424168\n",
      "epoch: 193, loss: 0.05614706007327186\n",
      "epoch: 194, loss: 0.05635465273371665\n",
      "epoch: 195, loss: 0.0572548755153548\n",
      "epoch: 196, loss: 0.056483515043510124\n",
      "epoch: 197, loss: 0.0574396776210051\n",
      "epoch: 198, loss: 0.0567754777657683\n",
      "epoch: 199, loss: 0.05812986227829242\n",
      "epoch: 200, loss: 0.05647062246862333\n",
      "epoch: 201, loss: 0.057993272799649276\n",
      "epoch: 202, loss: 0.056491683921194635\n",
      "epoch: 203, loss: 0.057079893405898474\n",
      "epoch: 204, loss: 0.0574227586039342\n",
      "epoch: 205, loss: 0.056036398047581315\n",
      "epoch: 206, loss: 0.05763796661631204\n",
      "epoch: 207, loss: 0.05568329666857608\n",
      "epoch: 208, loss: 0.05665848067292245\n",
      "epoch: 209, loss: 0.05728709456161596\n",
      "epoch: 210, loss: 0.05714154918678105\n",
      "epoch: 211, loss: 0.0575081865536049\n",
      "epoch: 212, loss: 0.05623997712973505\n",
      "epoch: 213, loss: 0.05657112041808432\n",
      "epoch: 214, loss: 0.057020832849957515\n",
      "epoch: 215, loss: 0.0564104093646165\n",
      "epoch: 216, loss: 0.05563774964684853\n",
      "epoch: 217, loss: 0.057245904914452694\n",
      "epoch: 218, loss: 0.05464170628692955\n",
      "epoch: 219, loss: 0.055388331282301806\n",
      "epoch: 220, loss: 0.05626357415167149\n",
      "epoch: 221, loss: 0.05892556844628416\n",
      "epoch: 222, loss: 0.05637390923220664\n",
      "epoch: 223, loss: 0.05889510921406327\n",
      "epoch: 224, loss: 0.05586045133532025\n",
      "epoch: 225, loss: 0.054730971132812556\n",
      "epoch: 226, loss: 0.056246283420477994\n",
      "epoch: 227, loss: 0.05641678543179296\n",
      "epoch: 228, loss: 0.055851634591817856\n",
      "epoch: 229, loss: 0.056575440961751156\n",
      "epoch: 230, loss: 0.05563445503503317\n",
      "epoch: 231, loss: 0.056420291162794456\n",
      "epoch: 232, loss: 0.05392664120881818\n",
      "epoch: 233, loss: 0.05749877371999901\n",
      "epoch: 234, loss: 0.05438247953134123\n",
      "epoch: 235, loss: 0.05557414664508542\n",
      "epoch: 236, loss: 0.056670706886507105\n",
      "epoch: 237, loss: 0.05729233404417755\n",
      "epoch: 238, loss: 0.05743904710834613\n",
      "epoch: 239, loss: 0.056869997395551763\n",
      "epoch: 240, loss: 0.05542379215330584\n",
      "epoch: 241, loss: 0.055835214181570336\n",
      "epoch: 242, loss: 0.05580924532114295\n",
      "epoch: 243, loss: 0.05597871580539504\n",
      "epoch: 244, loss: 0.05460975776804844\n",
      "epoch: 245, loss: 0.05584439892118098\n",
      "epoch: 246, loss: 0.057258335647929925\n",
      "epoch: 247, loss: 0.054005023928766605\n",
      "epoch: 248, loss: 0.056870854015869554\n",
      "epoch: 249, loss: 0.055881389693240635\n",
      "epoch: 250, loss: 0.05546628552110633\n",
      "epoch: 251, loss: 0.05342164470494026\n",
      "epoch: 252, loss: 0.05490402669238392\n",
      "epoch: 253, loss: 0.05681605304562254\n",
      "epoch: 254, loss: 0.055478567694080994\n",
      "epoch: 255, loss: 0.0570806317118695\n",
      "epoch: 256, loss: 0.05648953574564075\n",
      "epoch: 257, loss: 0.055491212937340606\n",
      "epoch: 258, loss: 0.05462410159088904\n",
      "epoch: 259, loss: 0.05559127918240847\n",
      "epoch: 260, loss: 0.05568889411370037\n",
      "epoch: 261, loss: 0.05437973532389151\n",
      "epoch: 262, loss: 0.05482389903772855\n",
      "epoch: 263, loss: 0.0538460877869511\n",
      "epoch: 264, loss: 0.05508685157110449\n",
      "epoch: 265, loss: 0.05490921142336447\n",
      "epoch: 266, loss: 0.056017224211245775\n",
      "epoch: 267, loss: 0.055530758552777115\n",
      "epoch: 268, loss: 0.05431706523086177\n",
      "epoch: 269, loss: 0.053849893738515675\n",
      "epoch: 270, loss: 0.05683346117439214\n",
      "epoch: 271, loss: 0.054823252401547506\n",
      "epoch: 272, loss: 0.054224484774749726\n",
      "epoch: 273, loss: 0.055624427302973345\n",
      "epoch: 274, loss: 0.053940747246087994\n",
      "epoch: 275, loss: 0.05527136411546962\n",
      "epoch: 276, loss: 0.0539627176040085\n",
      "epoch: 277, loss: 0.05504577172541758\n",
      "epoch: 278, loss: 0.05607578127091983\n",
      "epoch: 279, loss: 0.05487064392218599\n",
      "epoch: 280, loss: 0.05414743084838847\n",
      "epoch: 281, loss: 0.05516879186325241\n",
      "epoch: 282, loss: 0.05559203253505984\n",
      "epoch: 283, loss: 0.05499454516393598\n",
      "epoch: 284, loss: 0.05504135195951676\n",
      "epoch: 285, loss: 0.05570606480614515\n",
      "epoch: 286, loss: 0.055625708533625584\n",
      "epoch: 287, loss: 0.054650321188091766\n",
      "epoch: 288, loss: 0.053770180653373245\n",
      "epoch: 289, loss: 0.05471972731174901\n",
      "epoch: 290, loss: 0.05516269084182568\n",
      "epoch: 291, loss: 0.05451488826656714\n",
      "epoch: 292, loss: 0.057714133166882675\n",
      "epoch: 293, loss: 0.054145611844433006\n",
      "epoch: 294, loss: 0.0554941886512097\n",
      "epoch: 295, loss: 0.055591504722542595\n",
      "epoch: 296, loss: 0.056585998288937844\n",
      "epoch: 297, loss: 0.054898245791264344\n",
      "epoch: 298, loss: 0.05392793754435843\n",
      "epoch: 299, loss: 0.0536691547385999\n",
      "epoch: 300, loss: 0.05427361135662068\n",
      "epoch: 301, loss: 0.054373386839870363\n",
      "epoch: 302, loss: 0.05461938747612294\n",
      "epoch: 303, loss: 0.05375064477266278\n",
      "epoch: 304, loss: 0.055308728900854476\n",
      "epoch: 305, loss: 0.05597197825409239\n",
      "epoch: 306, loss: 0.05557780260278378\n",
      "epoch: 307, loss: 0.05459821013209876\n",
      "epoch: 308, loss: 0.052538925847329665\n",
      "epoch: 309, loss: 0.053774238855112344\n",
      "epoch: 310, loss: 0.05432058052974753\n",
      "epoch: 311, loss: 0.05312655571469804\n",
      "epoch: 312, loss: 0.05504347167152446\n",
      "epoch: 313, loss: 0.0555287234892603\n",
      "epoch: 314, loss: 0.05561966253299033\n",
      "epoch: 315, loss: 0.05447759314120049\n",
      "epoch: 316, loss: 0.053982779099897016\n",
      "epoch: 317, loss: 0.054388180185924284\n",
      "epoch: 318, loss: 0.05459456804965157\n",
      "epoch: 319, loss: 0.05360084163839929\n",
      "epoch: 320, loss: 0.05404258290218422\n",
      "epoch: 321, loss: 0.053422607845277525\n",
      "epoch: 322, loss: 0.056590063984913286\n",
      "epoch: 323, loss: 0.054134304606122896\n",
      "epoch: 324, loss: 0.05443822812958388\n",
      "epoch: 325, loss: 0.05482424129877472\n",
      "epoch: 326, loss: 0.05390512428857619\n",
      "epoch: 327, loss: 0.052587381134799216\n",
      "epoch: 328, loss: 0.05308531384798698\n",
      "epoch: 329, loss: 0.05487046270718565\n",
      "epoch: 330, loss: 0.054474521850352176\n",
      "epoch: 331, loss: 0.05322762850119034\n",
      "epoch: 332, loss: 0.05511501260480145\n",
      "epoch: 333, loss: 0.05360408911656123\n",
      "epoch: 334, loss: 0.05340367988537764\n",
      "epoch: 335, loss: 0.054690684301021975\n",
      "epoch: 336, loss: 0.0550946305229445\n",
      "epoch: 337, loss: 0.053181370109086856\n",
      "epoch: 338, loss: 0.05453550558013376\n",
      "epoch: 339, loss: 0.05502563775371527\n",
      "epoch: 340, loss: 0.052863252465613186\n",
      "epoch: 341, loss: 0.052326983233797364\n",
      "epoch: 342, loss: 0.055046544592187274\n",
      "epoch: 343, loss: 0.05423831670486834\n",
      "epoch: 344, loss: 0.05429054684645962\n",
      "epoch: 345, loss: 0.05332015488966135\n",
      "epoch: 346, loss: 0.05409630559006473\n",
      "epoch: 347, loss: 0.05482090207806323\n",
      "epoch: 348, loss: 0.05379313453886425\n",
      "epoch: 349, loss: 0.05402972615411272\n",
      "epoch: 350, loss: 0.052987411778303795\n",
      "epoch: 351, loss: 0.05610589509160491\n",
      "epoch: 352, loss: 0.05394183635507943\n",
      "epoch: 353, loss: 0.05570886305940803\n",
      "epoch: 354, loss: 0.0528034745293553\n",
      "epoch: 355, loss: 0.05595668195019243\n",
      "epoch: 356, loss: 0.054116858103952836\n",
      "epoch: 357, loss: 0.05312529385992093\n",
      "epoch: 358, loss: 0.05302937638043659\n",
      "epoch: 359, loss: 0.05530859750433592\n",
      "epoch: 360, loss: 0.053574304409266915\n",
      "epoch: 361, loss: 0.05570929580426309\n",
      "epoch: 362, loss: 0.054301298419886734\n",
      "epoch: 363, loss: 0.05296902330883313\n",
      "epoch: 364, loss: 0.053702351564425044\n",
      "epoch: 365, loss: 0.05320035113254562\n",
      "epoch: 366, loss: 0.054132039775140584\n",
      "epoch: 367, loss: 0.05350889156397898\n",
      "epoch: 368, loss: 0.05451771147636464\n",
      "epoch: 369, loss: 0.054522709229786415\n",
      "epoch: 370, loss: 0.052257460229157005\n",
      "epoch: 371, loss: 0.053907188259472605\n",
      "epoch: 372, loss: 0.05351372531731613\n",
      "epoch: 373, loss: 0.053367035143310204\n",
      "epoch: 374, loss: 0.05533483020553831\n",
      "epoch: 375, loss: 0.05334463058534311\n",
      "epoch: 376, loss: 0.055452451793826185\n",
      "epoch: 377, loss: 0.054881925228983164\n",
      "epoch: 378, loss: 0.0520246222440619\n",
      "epoch: 379, loss: 0.05410200622281991\n",
      "epoch: 380, loss: 0.054124714522913564\n",
      "epoch: 381, loss: 0.054652656930556986\n",
      "epoch: 382, loss: 0.05410754749755142\n",
      "epoch: 383, loss: 0.05420349099586019\n",
      "epoch: 384, loss: 0.052320031718409155\n",
      "epoch: 385, loss: 0.05550304169446463\n",
      "epoch: 386, loss: 0.05488587856234517\n",
      "epoch: 387, loss: 0.054953808736172505\n",
      "epoch: 388, loss: 0.053557530722173396\n",
      "epoch: 389, loss: 0.05360444808320608\n",
      "epoch: 390, loss: 0.05527162041107658\n",
      "epoch: 391, loss: 0.052253616879170295\n",
      "epoch: 392, loss: 0.05272558091382962\n",
      "epoch: 393, loss: 0.05461073298647534\n",
      "epoch: 394, loss: 0.05322680282552028\n",
      "epoch: 395, loss: 0.055887981128762476\n",
      "epoch: 396, loss: 0.05460713405045681\n",
      "epoch: 397, loss: 0.054788406247098465\n",
      "epoch: 398, loss: 0.05353162648680154\n",
      "epoch: 399, loss: 0.05411639277008362\n",
      "tanh\n",
      "..Colorizer Test started..\n",
      "Image: 1, loss: 0.00026963185518980026\n",
      "Image: 2, loss: 0.00017915814532898366\n",
      "Image: 3, loss: 0.00051064882427454\n",
      "Image: 4, loss: 0.0002620861923787743\n",
      "Image: 5, loss: 0.0003609982959460467\n",
      "Image: 6, loss: 0.00026847742265090346\n",
      "Image: 7, loss: 0.0008229536470025778\n",
      "Image: 8, loss: 0.00030857283854857087\n",
      "Image: 9, loss: 0.0006721557583659887\n",
      "Image: 10, loss: 0.000484822376165539\n",
      "Image: 11, loss: 0.00021008765907026827\n",
      "Image: 12, loss: 9.144168143393472e-05\n",
      "Image: 13, loss: 0.00039773457683622837\n",
      "Image: 14, loss: 0.00032172241481021047\n",
      "Image: 15, loss: 0.00024344997655134648\n",
      "Image: 16, loss: 0.0005071370978839695\n",
      "Image: 17, loss: 0.00012574151332955807\n",
      "Image: 18, loss: 0.00015830827760510147\n",
      "Image: 19, loss: 0.00026191509095951915\n",
      "Image: 20, loss: 0.00023471811437048018\n",
      "Image: 21, loss: 0.00033195759169757366\n",
      "Image: 22, loss: 0.0009547286317683756\n",
      "Image: 23, loss: 0.00013202204718254507\n",
      "Image: 24, loss: 0.0002932856441475451\n",
      "Image: 25, loss: 0.0006080714520066977\n",
      "Image: 26, loss: 0.00019123965466860682\n",
      "Image: 27, loss: 0.00020375818712636828\n",
      "Image: 28, loss: 0.00011335116141708568\n",
      "Image: 29, loss: 0.00022435412392951548\n",
      "Image: 30, loss: 0.0004499195783864707\n",
      "Image: 31, loss: 0.00012038631393807009\n",
      "Image: 32, loss: 0.00023433749447576702\n",
      "Image: 33, loss: 0.00021329600713215768\n",
      "Image: 34, loss: 0.0002749476698227227\n",
      "Image: 35, loss: 0.0006163704674690962\n",
      "Image: 36, loss: 0.0002217286528320983\n",
      "Image: 37, loss: 0.00028865892090834677\n",
      "Image: 38, loss: 0.00011873965559061617\n",
      "Image: 39, loss: 0.0002811300219036639\n",
      "Image: 40, loss: 0.000723240606021136\n",
      "Image: 41, loss: 0.0007782059838064015\n",
      "Image: 42, loss: 0.0009099580347537994\n",
      "Image: 43, loss: 0.00043448511860333383\n",
      "Image: 44, loss: 0.0005216806894168258\n",
      "Image: 45, loss: 0.00015893069212324917\n",
      "Image: 46, loss: 0.0001719179272186011\n",
      "Image: 47, loss: 0.0009092657710425556\n",
      "Image: 48, loss: 0.0002530071069486439\n",
      "Image: 49, loss: 0.00046836028923280537\n",
      "Image: 50, loss: 0.0002632129762787372\n",
      "Image: 51, loss: 0.0002559657150413841\n",
      "Image: 52, loss: 0.00023998717369977385\n",
      "Image: 53, loss: 0.0001481112849432975\n",
      "Image: 54, loss: 0.00025436660507693887\n",
      "Image: 55, loss: 0.00034602519008331\n",
      "Image: 56, loss: 0.0003572109271772206\n",
      "Image: 57, loss: 0.0005669702077284455\n",
      "Image: 58, loss: 0.0005784870591014624\n",
      "Image: 59, loss: 0.0003417111001908779\n",
      "Image: 60, loss: 0.0003234043251723051\n",
      "Image: 61, loss: 0.0010645197471603751\n",
      "Image: 62, loss: 0.0001915161410579458\n",
      "Image: 63, loss: 0.00022279903350863606\n",
      "Image: 64, loss: 0.00025754969101399183\n",
      "Image: 65, loss: 0.00032111589098349214\n",
      "Image: 66, loss: 0.00024943117750808597\n",
      "Image: 67, loss: 0.00020489930466283113\n",
      "Image: 68, loss: 0.0003537516458891332\n",
      "Image: 69, loss: 0.00033900164999067783\n",
      "Image: 70, loss: 0.0003609602863434702\n",
      "Image: 71, loss: 0.00018721388187259436\n",
      "Image: 72, loss: 0.0003816382377408445\n",
      "Image: 73, loss: 0.0004103371175006032\n",
      "Image: 74, loss: 0.000568466610275209\n",
      "Image: 75, loss: 0.00017275704885832965\n",
      "Image: 76, loss: 0.00020358781330287457\n",
      "..Regressor training started..\n",
      "epoch: 0, loss: 0.8300751339120325\n",
      "epoch: 1, loss: 0.14847559473855654\n",
      "epoch: 2, loss: 0.12493249878752977\n",
      "epoch: 3, loss: 0.10838787810644135\n",
      "epoch: 4, loss: 0.10693842657929054\n",
      "epoch: 5, loss: 0.0985143791695009\n",
      "epoch: 6, loss: 0.09623029854992637\n",
      "epoch: 7, loss: 0.08944029268604936\n",
      "epoch: 8, loss: 0.08931692523037782\n",
      "epoch: 9, loss: 0.08243790933920536\n",
      "epoch: 10, loss: 0.07737236329558073\n",
      "epoch: 11, loss: 0.07888139568240149\n",
      "epoch: 12, loss: 0.07963493271381594\n",
      "epoch: 13, loss: 0.0758154283685144\n",
      "epoch: 14, loss: 0.07362657457997557\n",
      "epoch: 15, loss: 0.07361805072287098\n",
      "epoch: 16, loss: 0.07185768366616685\n",
      "epoch: 17, loss: 0.08037812224938534\n",
      "epoch: 18, loss: 0.07127531101286877\n",
      "epoch: 19, loss: 0.06856527114359778\n",
      "epoch: 20, loss: 0.07133426189102465\n",
      "epoch: 21, loss: 0.06934888401519856\n",
      "epoch: 22, loss: 0.06580321160072344\n",
      "epoch: 23, loss: 0.06468063162901672\n",
      "epoch: 24, loss: 0.06622582630734541\n",
      "epoch: 25, loss: 0.06772358345187968\n",
      "epoch: 26, loss: 0.06485244210125529\n",
      "epoch: 27, loss: 0.06304640601229039\n",
      "epoch: 28, loss: 0.06195997292525135\n",
      "epoch: 29, loss: 0.06144621659768745\n",
      "epoch: 30, loss: 0.05789672121682088\n",
      "epoch: 31, loss: 0.05800949786862475\n",
      "epoch: 32, loss: 0.057181159041647334\n",
      "epoch: 33, loss: 0.055178975830131094\n",
      "epoch: 34, loss: 0.056527781511249486\n",
      "epoch: 35, loss: 0.05169816257148341\n",
      "epoch: 36, loss: 0.052187093500833726\n",
      "epoch: 37, loss: 0.050200745899928734\n",
      "epoch: 38, loss: 0.0481774325708102\n",
      "epoch: 39, loss: 0.04753320468807942\n",
      "epoch: 40, loss: 0.04552149379742332\n",
      "epoch: 41, loss: 0.04642388060892699\n",
      "epoch: 42, loss: 0.044175201197504066\n",
      "epoch: 43, loss: 0.04323720316460822\n",
      "epoch: 44, loss: 0.042833472338315914\n",
      "epoch: 45, loss: 0.040649598126037745\n",
      "epoch: 46, loss: 0.03802126588925603\n",
      "epoch: 47, loss: 0.03941614272480365\n",
      "epoch: 48, loss: 0.03865369582126732\n",
      "epoch: 49, loss: 0.03865573819530255\n",
      "epoch: 50, loss: 0.036223740062268917\n",
      "epoch: 51, loss: 0.036866876054773456\n",
      "epoch: 52, loss: 0.03590293213892437\n",
      "epoch: 53, loss: 0.03465137181956379\n",
      "epoch: 54, loss: 0.03555379081808496\n",
      "epoch: 55, loss: 0.03657085090344481\n",
      "epoch: 56, loss: 0.03404969309303851\n",
      "epoch: 57, loss: 0.03240029272092215\n",
      "epoch: 58, loss: 0.03280499191896524\n",
      "epoch: 59, loss: 0.03245278938447882\n",
      "epoch: 60, loss: 0.03125942562292039\n",
      "epoch: 61, loss: 0.03057498527050484\n",
      "epoch: 62, loss: 0.03083652994791919\n",
      "epoch: 63, loss: 0.029870531092456076\n",
      "epoch: 64, loss: 0.03019588581082644\n",
      "epoch: 65, loss: 0.030050649751501624\n",
      "epoch: 66, loss: 0.029169045770686353\n",
      "epoch: 67, loss: 0.029346377099500387\n",
      "epoch: 68, loss: 0.027151354992383858\n",
      "epoch: 69, loss: 0.029325732441066066\n",
      "epoch: 70, loss: 0.02722611613899062\n",
      "epoch: 71, loss: 0.02822554777321784\n",
      "epoch: 72, loss: 0.029087155929119035\n",
      "epoch: 73, loss: 0.027497997847603983\n",
      "epoch: 74, loss: 0.027516651834957884\n",
      "epoch: 75, loss: 0.026896747049249825\n",
      "epoch: 76, loss: 0.027207078368519433\n",
      "epoch: 77, loss: 0.025486635706329253\n",
      "epoch: 78, loss: 0.027158252436493058\n",
      "epoch: 79, loss: 0.026731380847195396\n",
      "epoch: 80, loss: 0.02598565196240088\n",
      "epoch: 81, loss: 0.02544329014926916\n",
      "epoch: 82, loss: 0.02639361412911967\n",
      "epoch: 83, loss: 0.024296742711158004\n",
      "epoch: 84, loss: 0.024536624061511247\n",
      "epoch: 85, loss: 0.02471935682297044\n",
      "epoch: 86, loss: 0.024781959878055204\n",
      "epoch: 87, loss: 0.023801088053005515\n",
      "epoch: 88, loss: 0.024738944260207063\n",
      "epoch: 89, loss: 0.024611073295091046\n",
      "epoch: 90, loss: 0.02466592370001308\n",
      "epoch: 91, loss: 0.024451175176182005\n",
      "epoch: 92, loss: 0.023720878532913048\n",
      "epoch: 93, loss: 0.023858870637923246\n",
      "epoch: 94, loss: 0.024107286362777813\n",
      "epoch: 95, loss: 0.023156832403401495\n",
      "epoch: 96, loss: 0.02378510210201057\n",
      "epoch: 97, loss: 0.023197910515591502\n",
      "epoch: 98, loss: 0.023212129063722386\n",
      "epoch: 99, loss: 0.021830972193129128\n",
      "epoch: 100, loss: 0.023169809072896896\n",
      "epoch: 101, loss: 0.02311155510142271\n",
      "epoch: 102, loss: 0.022663366909000615\n",
      "epoch: 103, loss: 0.02293445765008073\n",
      "epoch: 104, loss: 0.022417115048483538\n",
      "epoch: 105, loss: 0.022125255900391494\n",
      "epoch: 106, loss: 0.02227347782900324\n",
      "epoch: 107, loss: 0.022563476372852165\n",
      "epoch: 108, loss: 0.022128059161332203\n",
      "epoch: 109, loss: 0.024749097303356393\n",
      "epoch: 110, loss: 0.023433112883139984\n",
      "epoch: 111, loss: 0.022220268539967947\n",
      "epoch: 112, loss: 0.02228877851848665\n",
      "epoch: 113, loss: 0.02229096036990086\n",
      "epoch: 114, loss: 0.02190695259923814\n",
      "epoch: 115, loss: 0.02148535235755844\n",
      "epoch: 116, loss: 0.021263036027448834\n",
      "epoch: 117, loss: 0.020932847966832924\n",
      "epoch: 118, loss: 0.021160738009712077\n",
      "epoch: 119, loss: 0.02152043760088418\n",
      "epoch: 120, loss: 0.0211625236152031\n",
      "epoch: 121, loss: 0.02207581402581127\n",
      "epoch: 122, loss: 0.022080017571170174\n",
      "epoch: 123, loss: 0.021507741903405986\n",
      "epoch: 124, loss: 0.021099321910696744\n",
      "epoch: 125, loss: 0.020701686156826327\n",
      "epoch: 126, loss: 0.021634504123539955\n",
      "epoch: 127, loss: 0.021482251632733096\n",
      "epoch: 128, loss: 0.021014536372604198\n",
      "epoch: 129, loss: 0.02158020058141119\n",
      "epoch: 130, loss: 0.02097041411252576\n",
      "epoch: 131, loss: 0.022098605973951635\n",
      "epoch: 132, loss: 0.022499801538515385\n",
      "epoch: 133, loss: 0.022092295932452544\n",
      "epoch: 134, loss: 0.020712278401333606\n",
      "epoch: 135, loss: 0.020169051483208023\n",
      "epoch: 136, loss: 0.02073874194866221\n",
      "epoch: 137, loss: 0.021328718531549384\n",
      "epoch: 138, loss: 0.021951120283119963\n",
      "epoch: 139, loss: 0.02103622250888293\n",
      "epoch: 140, loss: 0.020964295682460943\n",
      "epoch: 141, loss: 0.02010416845587315\n",
      "epoch: 142, loss: 0.020551630146655953\n",
      "epoch: 143, loss: 0.021309489602117537\n",
      "epoch: 144, loss: 0.020084419639715634\n",
      "epoch: 145, loss: 0.02014135894660285\n",
      "epoch: 146, loss: 0.020909346140797425\n",
      "epoch: 147, loss: 0.020040346717905777\n",
      "epoch: 148, loss: 0.020650002900765685\n",
      "epoch: 149, loss: 0.02112956697055779\n",
      "epoch: 150, loss: 0.019259931909800798\n",
      "epoch: 151, loss: 0.020709610479570983\n",
      "epoch: 152, loss: 0.02096509843977401\n",
      "epoch: 153, loss: 0.020234673689628835\n",
      "epoch: 154, loss: 0.01982725253856188\n",
      "epoch: 155, loss: 0.020981554333047825\n",
      "epoch: 156, loss: 0.019544943032997253\n",
      "epoch: 157, loss: 0.019900732419955602\n",
      "epoch: 158, loss: 0.020765055182891956\n",
      "epoch: 159, loss: 0.021034353458162514\n",
      "epoch: 160, loss: 0.020247114314770442\n",
      "epoch: 161, loss: 0.019721135657164268\n",
      "epoch: 162, loss: 0.02034501840626035\n",
      "epoch: 163, loss: 0.020618483808902965\n",
      "epoch: 164, loss: 0.020558980337227695\n",
      "epoch: 165, loss: 0.019236451025790302\n",
      "epoch: 166, loss: 0.019865442985064874\n",
      "epoch: 167, loss: 0.020221842119099165\n",
      "epoch: 168, loss: 0.01940333793118043\n",
      "epoch: 169, loss: 0.01989627475177258\n",
      "epoch: 170, loss: 0.019711096980245202\n",
      "epoch: 171, loss: 0.02115981641691178\n",
      "epoch: 172, loss: 0.02052197403008904\n",
      "epoch: 173, loss: 0.020107167922105873\n",
      "epoch: 174, loss: 0.019064653442001145\n",
      "epoch: 175, loss: 0.019887341554749582\n",
      "epoch: 176, loss: 0.019157097435709147\n",
      "epoch: 177, loss: 0.01977854093456699\n",
      "epoch: 178, loss: 0.01994039286455518\n",
      "epoch: 179, loss: 0.020592675681655237\n",
      "epoch: 180, loss: 0.019731608361325925\n",
      "epoch: 181, loss: 0.01960124815650488\n",
      "epoch: 182, loss: 0.018927558071482053\n",
      "epoch: 183, loss: 0.019507937294292788\n",
      "epoch: 184, loss: 0.020457953742152313\n",
      "epoch: 185, loss: 0.019485802042254363\n",
      "epoch: 186, loss: 0.020658892332903633\n",
      "epoch: 187, loss: 0.018679013111068343\n",
      "epoch: 188, loss: 0.019665321178763406\n",
      "epoch: 189, loss: 0.019772597681367188\n",
      "epoch: 190, loss: 0.01963573707962496\n",
      "epoch: 191, loss: 0.019661285463371314\n",
      "epoch: 192, loss: 0.018914897795184515\n",
      "epoch: 193, loss: 0.02102523227040365\n",
      "epoch: 194, loss: 0.01989284747378406\n",
      "epoch: 195, loss: 0.020261455181753263\n",
      "epoch: 196, loss: 0.02023392038881866\n",
      "epoch: 197, loss: 0.019168513958902622\n",
      "epoch: 198, loss: 0.019083047808635456\n",
      "epoch: 199, loss: 0.019351761325197003\n",
      "epoch: 200, loss: 0.01897555580308108\n",
      "epoch: 201, loss: 0.018372885167991626\n",
      "epoch: 202, loss: 0.01908065813313442\n",
      "epoch: 203, loss: 0.019850682683681953\n",
      "epoch: 204, loss: 0.019120891081911395\n",
      "epoch: 205, loss: 0.019334788131345704\n",
      "epoch: 206, loss: 0.021062261055703857\n",
      "epoch: 207, loss: 0.0180996817034611\n",
      "epoch: 208, loss: 0.018433494616147073\n",
      "epoch: 209, loss: 0.018379535417807347\n",
      "epoch: 210, loss: 0.018161262024477764\n",
      "epoch: 211, loss: 0.01877587581930129\n",
      "epoch: 212, loss: 0.01878246213982493\n",
      "epoch: 213, loss: 0.01879518855639617\n",
      "epoch: 214, loss: 0.01914293958361668\n",
      "epoch: 215, loss: 0.019575876131966652\n",
      "epoch: 216, loss: 0.020184468642582942\n",
      "epoch: 217, loss: 0.01946241416681005\n",
      "epoch: 218, loss: 0.01822512762100814\n",
      "epoch: 219, loss: 0.019406515715672867\n",
      "epoch: 220, loss: 0.02034833367270039\n",
      "epoch: 221, loss: 0.019378600725758588\n",
      "epoch: 222, loss: 0.019440960565589194\n",
      "epoch: 223, loss: 0.018483979800294037\n",
      "epoch: 224, loss: 0.01882309563097806\n",
      "epoch: 225, loss: 0.018478419797247625\n",
      "epoch: 226, loss: 0.019282537953586143\n",
      "epoch: 227, loss: 0.01880755044385296\n",
      "epoch: 228, loss: 0.019446994946520135\n",
      "epoch: 229, loss: 0.018539493526077422\n",
      "epoch: 230, loss: 0.018738217593636364\n",
      "epoch: 231, loss: 0.01966765176894114\n",
      "epoch: 232, loss: 0.019006229666047147\n",
      "epoch: 233, loss: 0.018455106376677577\n",
      "epoch: 234, loss: 0.017984033564061974\n",
      "epoch: 235, loss: 0.01844875948427216\n",
      "epoch: 236, loss: 0.01956845379936567\n",
      "epoch: 237, loss: 0.01830293358580093\n",
      "epoch: 238, loss: 0.018573310801912157\n",
      "epoch: 239, loss: 0.017987047353926755\n",
      "epoch: 240, loss: 0.019521096503012814\n",
      "epoch: 241, loss: 0.018718711377005093\n",
      "epoch: 242, loss: 0.019113065332021506\n",
      "epoch: 243, loss: 0.01784104425860278\n",
      "epoch: 244, loss: 0.020021194873152126\n",
      "epoch: 245, loss: 0.019165787292877212\n",
      "epoch: 246, loss: 0.0189447952707269\n",
      "epoch: 247, loss: 0.019790423225458653\n",
      "epoch: 248, loss: 0.01816009683352604\n",
      "epoch: 249, loss: 0.018216421756733325\n",
      "epoch: 250, loss: 0.01820494635057912\n",
      "epoch: 251, loss: 0.017680670632671536\n",
      "epoch: 252, loss: 0.01798460664213053\n",
      "epoch: 253, loss: 0.018048946032649837\n",
      "epoch: 254, loss: 0.017916658935973828\n",
      "epoch: 255, loss: 0.018708362104007392\n",
      "epoch: 256, loss: 0.018606461484523606\n",
      "epoch: 257, loss: 0.019386395611945773\n",
      "epoch: 258, loss: 0.020156108127594052\n",
      "epoch: 259, loss: 0.01710839048246271\n",
      "epoch: 260, loss: 0.019031287528378016\n",
      "epoch: 261, loss: 0.01741466861722074\n",
      "epoch: 262, loss: 0.01805361031983921\n",
      "epoch: 263, loss: 0.01940433806976216\n",
      "epoch: 264, loss: 0.01896197500263952\n",
      "epoch: 265, loss: 0.018505707001168048\n",
      "epoch: 266, loss: 0.017717048647682532\n",
      "epoch: 267, loss: 0.019532706835889257\n",
      "epoch: 268, loss: 0.018712873980803124\n",
      "epoch: 269, loss: 0.018413011321172235\n",
      "epoch: 270, loss: 0.017875325331260683\n",
      "epoch: 271, loss: 0.01770472421321756\n",
      "epoch: 272, loss: 0.01851801308021095\n",
      "epoch: 273, loss: 0.019876554726579343\n",
      "epoch: 274, loss: 0.019024769403586106\n",
      "epoch: 275, loss: 0.018379433920017618\n",
      "epoch: 276, loss: 0.017947866168469773\n",
      "epoch: 277, loss: 0.018341773477004608\n",
      "epoch: 278, loss: 0.019024223407541285\n",
      "epoch: 279, loss: 0.01913948081073613\n",
      "epoch: 280, loss: 0.018420969983708346\n",
      "epoch: 281, loss: 0.018829181909495674\n",
      "epoch: 282, loss: 0.018772140870169096\n",
      "epoch: 283, loss: 0.018447515223670052\n",
      "epoch: 284, loss: 0.019008230773124524\n",
      "epoch: 285, loss: 0.0186354492107057\n",
      "epoch: 286, loss: 0.018683074796626897\n",
      "epoch: 287, loss: 0.01750929181707761\n",
      "epoch: 288, loss: 0.018353787298110547\n",
      "epoch: 289, loss: 0.017690157749711943\n",
      "epoch: 290, loss: 0.018505798589103506\n",
      "epoch: 291, loss: 0.01813642010802141\n",
      "epoch: 292, loss: 0.01811215431553137\n",
      "epoch: 293, loss: 0.01852271546886186\n",
      "epoch: 294, loss: 0.019488322239794797\n",
      "epoch: 295, loss: 0.018546641387729323\n",
      "epoch: 296, loss: 0.017567328181939956\n",
      "epoch: 297, loss: 0.016953125687450665\n",
      "epoch: 298, loss: 0.019074159569754556\n",
      "epoch: 299, loss: 0.018479771408237866\n",
      "epoch: 300, loss: 0.01843835771887825\n",
      "epoch: 301, loss: 0.01813353051784361\n",
      "epoch: 302, loss: 0.018038835616607685\n",
      "epoch: 303, loss: 0.018066653008645517\n",
      "epoch: 304, loss: 0.017951678536519466\n",
      "epoch: 305, loss: 0.01754035201338411\n",
      "epoch: 306, loss: 0.018205173415481113\n",
      "epoch: 307, loss: 0.019077887091953016\n",
      "epoch: 308, loss: 0.017685361567146174\n",
      "epoch: 309, loss: 0.019014776406038436\n",
      "epoch: 310, loss: 0.01849467453394027\n",
      "epoch: 311, loss: 0.01846702777174869\n",
      "epoch: 312, loss: 0.01738116088199604\n",
      "epoch: 313, loss: 0.01873656703355664\n",
      "epoch: 314, loss: 0.01847518597423914\n",
      "epoch: 315, loss: 0.01826617313327006\n",
      "epoch: 316, loss: 0.0187971077120892\n",
      "epoch: 317, loss: 0.018169717644013872\n",
      "epoch: 318, loss: 0.019040776917790936\n",
      "epoch: 319, loss: 0.018748994237284933\n",
      "epoch: 320, loss: 0.01730937840511615\n",
      "epoch: 321, loss: 0.018309035495803982\n",
      "epoch: 322, loss: 0.01749947859934764\n",
      "epoch: 323, loss: 0.017467803806084703\n",
      "epoch: 324, loss: 0.019657100169752084\n",
      "epoch: 325, loss: 0.018602893390379904\n",
      "epoch: 326, loss: 0.017898207887810713\n",
      "epoch: 327, loss: 0.01761813222674391\n",
      "epoch: 328, loss: 0.018080609161188477\n",
      "epoch: 329, loss: 0.018422551785988617\n",
      "epoch: 330, loss: 0.01949810543919739\n",
      "epoch: 331, loss: 0.01685906428974704\n",
      "epoch: 332, loss: 0.017051558908860898\n",
      "epoch: 333, loss: 0.018324657611628936\n",
      "epoch: 334, loss: 0.017391758886333264\n",
      "epoch: 335, loss: 0.01607592670916347\n",
      "epoch: 336, loss: 0.01804554286354687\n",
      "epoch: 337, loss: 0.017624655091822206\n",
      "epoch: 338, loss: 0.018753088507764915\n",
      "epoch: 339, loss: 0.017948380223970162\n",
      "epoch: 340, loss: 0.017882796786580002\n",
      "epoch: 341, loss: 0.019229761068345397\n",
      "epoch: 342, loss: 0.01680509293328214\n",
      "epoch: 343, loss: 0.01712810518074548\n",
      "epoch: 344, loss: 0.019852888833156612\n",
      "epoch: 345, loss: 0.018091328284754127\n",
      "epoch: 346, loss: 0.018486942094568803\n",
      "epoch: 347, loss: 0.01815509821244632\n",
      "epoch: 348, loss: 0.01673241248499835\n",
      "epoch: 349, loss: 0.018510929734475212\n",
      "epoch: 350, loss: 0.017807774614993832\n",
      "epoch: 351, loss: 0.01753487871883408\n",
      "epoch: 352, loss: 0.0179136561064297\n",
      "epoch: 353, loss: 0.016635071801829326\n",
      "epoch: 354, loss: 0.019448730664407776\n",
      "epoch: 355, loss: 0.018474739373232296\n",
      "epoch: 356, loss: 0.01852410543233418\n",
      "epoch: 357, loss: 0.01846689212379715\n",
      "epoch: 358, loss: 0.018059532206279982\n",
      "epoch: 359, loss: 0.01852018268800748\n",
      "epoch: 360, loss: 0.017197400075019686\n",
      "epoch: 361, loss: 0.017587654493581795\n",
      "epoch: 362, loss: 0.017457719584854203\n",
      "epoch: 363, loss: 0.01760955259851471\n",
      "epoch: 364, loss: 0.01787271907323884\n",
      "epoch: 365, loss: 0.017508912564153434\n",
      "epoch: 366, loss: 0.020123292442804086\n",
      "epoch: 367, loss: 0.017288567990362935\n",
      "epoch: 368, loss: 0.01711971716122207\n",
      "epoch: 369, loss: 0.018423744848405477\n",
      "epoch: 370, loss: 0.018082077881445002\n",
      "epoch: 371, loss: 0.017675361295005132\n",
      "epoch: 372, loss: 0.01744129168218933\n",
      "epoch: 373, loss: 0.017388528069659515\n",
      "epoch: 374, loss: 0.017996240521824802\n",
      "epoch: 375, loss: 0.01739912228913454\n",
      "epoch: 376, loss: 0.017504939915852447\n",
      "epoch: 377, loss: 0.019404291388127604\n",
      "epoch: 378, loss: 0.01749695249236538\n",
      "epoch: 379, loss: 0.01793338919105736\n",
      "epoch: 380, loss: 0.01788699287135387\n",
      "epoch: 381, loss: 0.01797613073085813\n",
      "epoch: 382, loss: 0.01693223680013034\n",
      "epoch: 383, loss: 0.01787160170442803\n",
      "epoch: 384, loss: 0.017279826267440512\n",
      "epoch: 385, loss: 0.017313573116553016\n",
      "epoch: 386, loss: 0.017217968887052848\n",
      "epoch: 387, loss: 0.01679492947914696\n",
      "epoch: 388, loss: 0.01715490173864964\n",
      "epoch: 389, loss: 0.0180180574516271\n",
      "epoch: 390, loss: 0.018254588906529534\n",
      "epoch: 391, loss: 0.018879940698298014\n",
      "epoch: 392, loss: 0.01760185043394813\n",
      "epoch: 393, loss: 0.017609248462576943\n",
      "epoch: 394, loss: 0.018121628318112926\n",
      "epoch: 395, loss: 0.017804507816435944\n",
      "epoch: 396, loss: 0.019471164044261968\n",
      "epoch: 397, loss: 0.017606667846848723\n",
      "epoch: 398, loss: 0.01770385512827488\n",
      "epoch: 399, loss: 0.018028344079539238\n",
      "..Regressor testing started..\n",
      "MSE: 0.00010690071370109571\n",
      "Image_num || Mean a || Mean b\n",
      "Image: 1 mean_a: 13.831356585025787 mean_b:11.217355251312256\n",
      "Image: 2 mean_a: 11.24070119857788 mean_b:9.842239439487457\n",
      "Image: 3 mean_a: 12.38849800825119 mean_b:10.28876107931137\n",
      "Image: 4 mean_a: 12.763492286205292 mean_b:7.959090888500214\n",
      "Image: 5 mean_a: 13.396249532699585 mean_b:9.128227353096008\n",
      "Image: 6 mean_a: 12.916426479816437 mean_b:10.962677717208862\n",
      "Image: 7 mean_a: 11.618142545223236 mean_b:14.428000569343567\n",
      "Image: 8 mean_a: 13.233952641487122 mean_b:15.21372240781784\n",
      "Image: 9 mean_a: 11.055225551128387 mean_b:8.803952753543854\n",
      "Image: 10 mean_a: 12.55956482887268 mean_b:11.305267333984375\n",
      "Image: 11 mean_a: 14.109851241111755 mean_b:11.554564356803894\n",
      "Image: 12 mean_a: 14.033171355724335 mean_b:11.07411813735962\n",
      "Image: 13 mean_a: 12.488843023777008 mean_b:9.36387550830841\n",
      "Image: 14 mean_a: 13.656717956066132 mean_b:10.65255355834961\n",
      "Image: 15 mean_a: 11.70876008272171 mean_b:9.312608659267426\n",
      "Image: 16 mean_a: 13.455617547035217 mean_b:12.22410362958908\n",
      "Image: 17 mean_a: 12.87759256362915 mean_b:11.622915089130402\n",
      "Image: 18 mean_a: 13.126631200313568 mean_b:11.326667785644531\n",
      "Image: 19 mean_a: 12.010448694229126 mean_b:9.356473505496979\n",
      "Image: 20 mean_a: 10.69810551404953 mean_b:9.479632496833801\n",
      "Image: 21 mean_a: 12.757686197757721 mean_b:12.717590749263763\n",
      "Image: 22 mean_a: 12.516049563884735 mean_b:9.551707029342651\n",
      "Image: 23 mean_a: 11.402055740356445 mean_b:10.757929503917694\n",
      "Image: 24 mean_a: 10.732258081436157 mean_b:9.85310685634613\n",
      "Image: 25 mean_a: 12.158473551273346 mean_b:13.427696645259857\n",
      "Image: 26 mean_a: 13.547481417655945 mean_b:11.251021444797516\n",
      "Image: 27 mean_a: 14.330361008644104 mean_b:11.512432217597961\n",
      "Image: 28 mean_a: 11.757975041866302 mean_b:10.459478318691254\n",
      "Image: 29 mean_a: 10.033080399036407 mean_b:10.33578735589981\n",
      "Image: 30 mean_a: 14.20320463180542 mean_b:10.790182173252106\n",
      "Image: 31 mean_a: 13.776046752929688 mean_b:11.355455040931702\n",
      "Image: 32 mean_a: 12.722363293170929 mean_b:11.759403765201569\n",
      "Image: 33 mean_a: 11.222462177276611 mean_b:13.619282364845276\n",
      "Image: 34 mean_a: 11.287377893924713 mean_b:9.607670426368713\n",
      "Image: 35 mean_a: 12.008077621459961 mean_b:10.603946566581726\n",
      "Image: 36 mean_a: 12.843683183193207 mean_b:10.821310102939606\n",
      "Image: 37 mean_a: 13.498859226703644 mean_b:12.337854325771332\n",
      "Image: 38 mean_a: 11.332580268383026 mean_b:14.456757426261902\n",
      "Image: 39 mean_a: 13.43842726945877 mean_b:12.444172620773315\n",
      "Image: 40 mean_a: 12.323825478553772 mean_b:11.464919567108154\n",
      "Image: 41 mean_a: 14.792598605155945 mean_b:16.019963145256042\n",
      "Image: 42 mean_a: 12.9206822514534 mean_b:13.70401781797409\n",
      "Image: 43 mean_a: 10.763264417648315 mean_b:10.941094875335693\n",
      "Image: 44 mean_a: 12.100291073322296 mean_b:13.040360629558563\n",
      "Image: 45 mean_a: 13.064086556434631 mean_b:11.880875647068024\n",
      "Image: 46 mean_a: 12.768887996673584 mean_b:10.401311039924622\n",
      "Image: 47 mean_a: 11.33736801147461 mean_b:9.539942860603333\n",
      "Image: 48 mean_a: 13.297409236431122 mean_b:13.400718092918396\n",
      "Image: 49 mean_a: 11.478933215141296 mean_b:9.316058874130249\n",
      "Image: 50 mean_a: 13.354299783706665 mean_b:12.35443663597107\n",
      "Image: 51 mean_a: 13.348797678947449 mean_b:13.60368800163269\n",
      "Image: 52 mean_a: 11.061730802059174 mean_b:12.019401013851166\n",
      "Image: 53 mean_a: 11.4105064868927 mean_b:10.151041269302368\n",
      "Image: 54 mean_a: 11.435402750968933 mean_b:11.932157695293427\n",
      "Image: 55 mean_a: 12.247707962989807 mean_b:8.983941495418549\n",
      "Image: 56 mean_a: 12.644969046115875 mean_b:8.133212745189667\n",
      "Image: 57 mean_a: 10.836919665336609 mean_b:10.74914437532425\n",
      "Image: 58 mean_a: 12.083723962306976 mean_b:10.852635622024536\n",
      "Image: 59 mean_a: 10.903765678405762 mean_b:10.384470343589783\n",
      "Image: 60 mean_a: 12.386932492256165 mean_b:11.415218234062195\n",
      "Image: 61 mean_a: 13.233055889606476 mean_b:12.448185205459595\n",
      "Image: 62 mean_a: 11.871801733970642 mean_b:11.998578131198883\n",
      "Image: 63 mean_a: 11.000341296195984 mean_b:9.668710350990295\n",
      "Image: 64 mean_a: 12.271053910255432 mean_b:8.845780909061432\n",
      "Image: 65 mean_a: 12.507142841815948 mean_b:11.38579261302948\n",
      "Image: 66 mean_a: 13.308991014957428 mean_b:11.414975047111511\n",
      "Image: 67 mean_a: 11.65278148651123 mean_b:7.717423856258392\n",
      "Image: 68 mean_a: 11.848926961421967 mean_b:10.181910812854767\n",
      "Image: 69 mean_a: 10.853471577167511 mean_b:12.162227749824524\n",
      "Image: 70 mean_a: 14.035466432571411 mean_b:11.361033141613007\n",
      "Image: 71 mean_a: 11.580296576023102 mean_b:9.678179442882538\n",
      "Image: 72 mean_a: 13.31104290485382 mean_b:13.166787445545197\n",
      "Image: 73 mean_a: 11.455116093158722 mean_b:10.428365588188171\n",
      "Image: 74 mean_a: 11.726299941539764 mean_b:11.202900826931\n",
      "Image: 75 mean_a: 12.529060065746307 mean_b:10.903856873512268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import os\n",
    "import re\n",
    "from shutil import copy2\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import lab2rgb\n",
    "from skimage.color import rgb2lab, rgb2gray\n",
    "from torchvision import datasets\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def data_load():\n",
    "    image_list = glob.glob('face_images/*.jpg')\n",
    "    print(\"Length of given Image List\", len(image_list))\n",
    "    train_test_split()\n",
    "    training_image_list = glob.glob('data/train/class/*.jpg')\n",
    "    test_image_list = glob.glob('data/test/class/*.jpg')\n",
    "    print(\"Length of training Image List\", len(training_image_list))\n",
    "    print(\"Length of testing Image List\", len(test_image_list))\n",
    "    \n",
    "def train_test_split():\n",
    "        os.makedirs('data/train/class/', exist_ok=True)\n",
    "        os.makedirs('data/test/class/', exist_ok=True)\n",
    "        os.makedirs('Model/Colorizer/', exist_ok=True)\n",
    "        os.makedirs('Model/Regressor/', exist_ok=True)\n",
    "        os.makedirs('Plots/Colorizer/', exist_ok=True)\n",
    "        os.makedirs('outputs_sigmoid/gray/',exist_ok=True)\n",
    "        os.makedirs('outputs_sigmoid/color/',exist_ok=True)\n",
    "        os.makedirs('outputs_sigmoid/disp/',exist_ok=True)\n",
    "        os.makedirs('outputs_tanh/gray/',exist_ok=True)\n",
    "        os.makedirs('outputs_tanh/color/',exist_ok=True)\n",
    "\n",
    "\n",
    "        number_of_images = len(next(os.walk('face_images'))[2])\n",
    "        print(\"Number of images - \", number_of_images)\n",
    "\n",
    "        for i, file in enumerate(os.listdir('face_images')):\n",
    "            if i < (0.1 * number_of_images):\n",
    "                copy2('face_images/' + file, 'data/test/class/' + file)\n",
    "                continue\n",
    "            else: \n",
    "                copy2('face_images/' + file, 'data/train/class/' + file)\n",
    "\n",
    "        print(\"Training Set Size : \", len(next(os.walk('data/train/class'))[2]))\n",
    "        print(\"Test Set Size : \", len(next(os.walk('data/test/class'))[2]))\n",
    "def build_dataset(cuda=False, num_workers=1,\n",
    "                  activation_function=\"sigmoid\"):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(128),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(128)\n",
    "    ])\n",
    "\n",
    "    train_datasets = []\n",
    "    if activation_function == \"sigmoid\" or activation_function == 'tanh':\n",
    "        train_datasets.append(AugmentImageDataset('data/train'))\n",
    "    elif activation_function == \"relu\":\n",
    "        train_datasets.append(AugmentImageDataset_RELU('data/train'))\n",
    "\n",
    "    for i in range(9):\n",
    "        if activation_function == \"sigmoid\":\n",
    "            train_datasets.append(AugmentImageDataset('data/train', transform))\n",
    "        elif activation_function == \"relu\":\n",
    "            train_datasets.append(AugmentImageDataset_RELU('data/train', transform))\n",
    "        elif activation_function == \"tanh\":\n",
    "            train_datasets.append(AugmentImageDataset_Tanh('data/train',\n",
    "                                                                        transform))\n",
    "\n",
    "    augmented_dataset = ConcatDataset(train_datasets)\n",
    "    print(\"Length of Augmented Dataset\", len(augmented_dataset))\n",
    "\n",
    "    train_loader_args = dict(shuffle=True,\n",
    "                             batch_size=16,\n",
    "                             num_workers=num_workers, pin_memory=True) \\\n",
    "        if cuda else dict(shuffle=True, batch_size=32)\n",
    "\n",
    "    augmented_dataset_batch_train = DataLoader(dataset=augmented_dataset, **train_loader_args)\n",
    "    augmented_dataset_batch_test = DataLoader(dataset=AugmentImageDataset('data/test'))\n",
    "\n",
    "    return augmented_dataset_batch_train, augmented_dataset_batch_test\n",
    "\n",
    "def execution_colorizer_sigmoid():\n",
    "    activation_function = \"sigmoid\"\n",
    "    save_path = {'grayscale': 'outputs_sigmoid/gray/', 'colorized': 'outputs_sigmoid/color/'}\n",
    "    device, is_cuda_present, num_workers = get_device()\n",
    "    model_name = \"Model/Colorizer/Colorizer_sigmoid_epoch_{0}_lr_{1}_weight_decay_{2}.pth\"\n",
    "\n",
    "    print(\"Device: {0}\".format(device))\n",
    "    augmented_dataset_batch_train, \\\n",
    "    augmented_dataset_batch_test = build_dataset(is_cuda_present, num_workers,activation_function)\n",
    "\n",
    "    colorizer = Manage_Colorize()\n",
    "    colorizer.train(augmented_dataset_batch_train,activation_function, device)\n",
    "    colorizer.test(augmented_dataset_batch_test, activation_function,save_path, device)\n",
    "    train_regressor(augmented_dataset_batch_train, device)\n",
    "    test_regressor(augmented_dataset_batch_test, device)\n",
    "def execution_colorizer_tanh():\n",
    "    activation_function = \"tanh\"\n",
    "    save_path = {'grayscale': 'outputs_tanh/gray/', 'colorized': 'outputs_tanh/color/'}\n",
    "    device, is_cuda_present, num_workers = get_device()\n",
    "    model_name = \"Model/Colorizer/Colorizer_tanh_epoch_{0}_lr_{1}_weight_decay_{2}.pth\"\n",
    "\n",
    "    print(\"Device: {0}\".format(device))\n",
    "    augmented_dataset_batch_train, \\\n",
    "    augmented_dataset_batch_test = build_dataset(is_cuda_present, num_workers,activation_function)\n",
    "    colorizer = Manage_Colorize()\n",
    "    colorizer.train(augmented_dataset_batch_train,activation_function, device)\n",
    "    colorizer.test(augmented_dataset_batch_test, activation_function,save_path, device)\n",
    "    train_regressor(augmented_dataset_batch_train, device)\n",
    "    test_regressor(augmented_dataset_batch_test, device)\n",
    "    \n",
    "def train_regressor(augmented_dataset_batch_train, device):\n",
    "    data_loader = augmented_dataset_batch_train\n",
    "    saved_model_path = \"Model/Regressor/Regressor.pth\"\n",
    "    epochs = 400\n",
    "    lr = 0.0001\n",
    "    weight_decay = 1e-5\n",
    "    in_channel = 1\n",
    "    hidden_channel = 3\n",
    "    out_dims = 2\n",
    "    loss_plot_path = \"Model/Regressor/Regressor_Loss_plot.jpeg\"   \n",
    "\n",
    "    print(\"..Regressor training started..\")\n",
    "    model = Regressor(in_channel=in_channel,\n",
    "                        hidden_channel=hidden_channel,\n",
    "                        out_dims=out_dims,\n",
    "                        train_mode=\"regressor\").to(device)\n",
    "\n",
    "    lossF = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "    loss_train = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch in data_loader:\n",
    "            l_channel, a_channel, b_channel = batch\n",
    "            l_channel = l_channel.to(device)\n",
    "\n",
    "            a_b_mean = get_ab_mean(a_channel, b_channel)\n",
    "            a_b_mean_hat = model(l_channel)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                loss = lossF(a_b_mean_hat.float().cuda(),\n",
    "                                 a_b_mean.float().cuda()).to(device)\n",
    "            else:\n",
    "                loss = lossF(a_b_mean_hat.float(),\n",
    "                                a_b_mean.float()).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(\"epoch: {0}, loss: {1}\"\n",
    "                .format(epoch, total_loss))\n",
    "        loss_train.append(total_loss)\n",
    "\n",
    "    plot_loss_epoch(loss_train, loss_plot_path)\n",
    "    torch.save(model.state_dict(), saved_model_path)\n",
    "\n",
    "def test_regressor(augmented_dataset_batch_test, device):\n",
    "    data_loader = augmented_dataset_batch_test\n",
    "    saved_model_path = \"Model/Regressor/Regressor.pth\"\n",
    "    in_channel = 1\n",
    "    hidden_channel = 3\n",
    "    out_dims = 2\n",
    "\n",
    "    print(\"..Regressor testing started..\")\n",
    "\n",
    "    model = Regressor(in_channel=in_channel,\n",
    "                          hidden_channel=hidden_channel,\n",
    "                          out_dims=out_dims,\n",
    "                          train_mode=\"regressor\").to(device)\n",
    "    model.load_state_dict(torch.load(saved_model_path, map_location=device))\n",
    "\n",
    "    a_list = []\n",
    "    b_list = []\n",
    "    lossF = nn.MSELoss()\n",
    "    total_loss = 0\n",
    "    loss_train = []\n",
    "    for batch in data_loader:\n",
    "        l_channel, a_channel, b_channel = batch\n",
    "        l_channel = l_channel.to(device)\n",
    "\n",
    "        a_b_mean = get_ab_mean(a_channel, b_channel)\n",
    "        a_b_mean_hat = model(l_channel).detach()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            loss = lossF(a_b_mean_hat.float().cuda(),\n",
    "                             a_b_mean.float().cuda()).to(device)\n",
    "        else:\n",
    "            loss = lossF(a_b_mean_hat.float(),\n",
    "                             a_b_mean.float()).to(device)\n",
    "\n",
    "        loss_train.append(loss.item())\n",
    "\n",
    "        a_b_pred = a_b_mean_hat[0].cpu().numpy()\n",
    "        a_list.append(a_b_pred[0])\n",
    "        b_list.append(a_b_pred[1])\n",
    "\n",
    "    print(\"MSE:\", np.average(np.asarray(loss_train)))\n",
    "    print(\"Image_num || Mean a || Mean b\")\n",
    "    for i in range(1, len(a_list)):\n",
    "        print(\"Image: {0} mean_a: {1} mean_b:{2}\".format(\n",
    "            i, (a_list[i] * 255) - 128, (b_list[i] * 255) - 128\n",
    "        ))\n",
    "    \n",
    "def get_device():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    is_cuda_present = True if torch.cuda.is_available() else False\n",
    "    num_workers = 8 if is_cuda_present else 0\n",
    "    return device, is_cuda_present, num_workers\n",
    "\n",
    "class AugmentImageDataset(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        global img_a, img_b, img_gray\n",
    "        path, target = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        img_lab = rgb2lab(np.asarray(img))\n",
    "        img_lab = img_lab + 128\n",
    "        img_lab = img_lab / 255\n",
    "        img_a = torch.from_numpy(img_lab[:, :, 1:2].transpose((2, 0, 1))).float() \n",
    "        img_b = torch.from_numpy(img_lab[:, :, 2:3].transpose((2, 0, 1))).float()\n",
    "        img_gray = torch.from_numpy(rgb2gray(np.asarray(img))).unsqueeze(0).float()\n",
    "        return img_gray, img_a, img_b\n",
    "\n",
    "class AugmentImageDataset_Tanh(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        global img_a, img_b, img_gray\n",
    "        path, target = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        img_lab = rgb2lab(np.asarray(img))\n",
    "        img_lab = img_lab + 128\n",
    "        img_lab = img_lab / 255\n",
    "        img_a = torch.from_numpy(img_lab[:, :, 1:2].transpose((2, 0, 1))).float()\n",
    "        img_b = torch.from_numpy(img_lab[:, :, 2:3].transpose((2, 0, 1))).float()\n",
    "        img_gray = torch.from_numpy(rgb2gray(np.asarray(img))).unsqueeze(0).float()\n",
    "        return img_gray, img_a, img_b\n",
    "\n",
    "class AugmentImageDataset_RELU(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        global img_a, img_b, img_gray\n",
    "        path, target = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        img_lab = rgb2lab(np.asarray(img))\n",
    "        img_a = torch.from_numpy(img_lab[:, :, 1:2].transpose((2, 0, 1))).float() \n",
    "        img_b = torch.from_numpy(img_lab[:, :, 2:3].transpose((2, 0, 1))).float()\n",
    "        img_gray = torch.from_numpy(rgb2gray(np.asarray(img))).unsqueeze(0).float()\n",
    "\n",
    "        return img_gray, img_a, img_b\n",
    "    \n",
    "def get_ab_mean(a_channel, b_channel):\n",
    "    a_channel_mean = a_channel.mean(dim=(2, 3))\n",
    "    b_channel_mean = b_channel.mean(dim=(2, 3))\n",
    "    a_b_mean = torch.cat([a_channel_mean,\n",
    "                              b_channel_mean], dim=1)\n",
    "    return a_b_mean\n",
    "\n",
    "def plot_loss_epoch(train_loss_avg, fig_name):\n",
    "    plt.ion()\n",
    "    fig = plt.figure()\n",
    "    plt.plot(train_loss_avg)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.draw()\n",
    "    plt.savefig(fig_name, dpi=220)\n",
    "    plt.clf()\n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, in_channel=1, hidden_channel=3, out_dims=2,\n",
    "                 train_mode=\"regressor\"):\n",
    "        super(Regressor, self).__init__()\n",
    "        self.train_mode = train_mode\n",
    "\n",
    "        self.feature_maps = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channel, out_channels=32,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=256,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=512,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        if self.train_mode == \"regressor\":\n",
    "            self.lin = nn.Linear(in_features=512 * 2 * 2, out_features=out_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_maps = self.feature_maps(x)\n",
    "        if self.train_mode == \"regressor\":\n",
    "            y_hat = torch.sigmoid(self.lin(feature_maps.reshape(-1, 512 * 2 * 2)))\n",
    "            return y_hat\n",
    "\n",
    "        else:\n",
    "            return feature_maps\n",
    "\n",
    "class Colorizer(nn.Module):\n",
    "    def __init__(self, in_channel=3, hidden_channel=3, out_channel=2,\n",
    "                 activation_function=\"sigmoid\"):\n",
    "        super(Colorizer, self).__init__()\n",
    "        self.activation_function = activation_function\n",
    "        self.feature_maps = Regressor(in_channel=1, hidden_channel=3, out_dims=2,\n",
    "                                      train_mode=\"colorizer\")\n",
    "        self.up_sample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=256,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=256,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=32,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=out_channel,\n",
    "                               kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            return torch.sigmoid(self.up_sample(self.feature_maps(x)))\n",
    "        elif self.activation_function == \"tanh\":\n",
    "            return torch.tanh(self.up_sample(self.feature_maps(x)))\n",
    "        elif self.activation_function == \"relu\":\n",
    "            return torch.relu(self.up_sample(self.feature_maps(x)))\n",
    "\n",
    "def show_img(image):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    np_img = image.numpy()\n",
    "    plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "    plt.show()\n",
    "def to_rgb(grayscale_input, ab_input, activation_function=\"tanh\",\n",
    "               save_path=None, save_name=None, device=\"cpu\"):\n",
    "    plt.clf()\n",
    "    color_image = torch.cat((grayscale_input, ab_input), 0).numpy()  # combine channels\n",
    "    color_image = color_image.transpose((1, 2, 0))  # rescale for matplotlib\n",
    "    color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n",
    "    color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128\n",
    "    color_image = lab2rgb(color_image.astype(np.float64))\n",
    "    grayscale_input = grayscale_input.squeeze().numpy()\n",
    "    if save_path is not None and save_name is not None:\n",
    "        plt.imsave(arr=grayscale_input, fname='{}{}'.format(save_path['grayscale'], save_name), cmap='gray')\n",
    "        plt.imsave(arr=color_image, fname='{}{}'.format(save_path['colorized'], save_name))\n",
    "def show_img_tensor(image):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(image.permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "def show_output_image(gray, orig, recons, fig_name):\n",
    "    plt.clf()\n",
    "    f = plt.figure()\n",
    "    f.add_subplot(1, 3, 1)\n",
    "    plt.imshow(mpimg.imread(gray))\n",
    "    plt.axis('off')\n",
    "    f.add_subplot(1, 3, 2)\n",
    "    plt.imshow(mpimg.imread(orig))\n",
    "    plt.axis('off')\n",
    "    f.add_subplot(1, 3, 3)\n",
    "    plt.imshow(mpimg.imread(recons))\n",
    "    plt.axis('off')\n",
    "    plt.draw()\n",
    "    plt.savefig(fig_name, dpi=220)\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    \n",
    "class EarlyStopping_DCN:\n",
    "\n",
    "    def __init__(self, patience=7, verbose=False, delta=0,\n",
    "                 model_path=None,\n",
    "                 trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.model_path = model_path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            self.trace_func(\n",
    "                f'Validation loss decreased ({self.val_loss_min} --> {val_loss}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.model_path)\n",
    "        self.val_loss_min = val_loss\n",
    "        \n",
    "class Manage_Colorize:\n",
    "    def train(self,augmented_dataset_batch_train, activation_function, device):\n",
    "        print(\"Activation Function: \", activation_function)\n",
    "\n",
    "        train_data_loader = augmented_dataset_batch_train\n",
    "        saved_model_path =\"Model/Colorizer/Colorizer_{0}.pth\".format(activation_function)\n",
    "\n",
    "        epochs = 400\n",
    "        lr = .0001\n",
    "        weight_decay = 1e-5\n",
    "        in_channel = 3\n",
    "        hidden_channel = 3\n",
    "        loss_plot_path = \"Model/Colorizer/Colorizer_Loss_plot_{0}.jpeg\".format(activation_function)\n",
    "\n",
    "        print(\"..Colorizer Training started..\")\n",
    "        model = Colorizer(in_channel=3, hidden_channel=3,\n",
    "                          activation_function=activation_function).to(device)\n",
    "\n",
    "        lossF = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr,\n",
    "                               weight_decay=weight_decay)\n",
    "        loss_train = []\n",
    "        early_stopping = EarlyStopping_DCN(patience=50, verbose=True,\n",
    "                                           model_path=saved_model_path)\n",
    "        for epoch in range(epochs):\n",
    "            total_loss_train = 0\n",
    "            total_loss_val = 0\n",
    "            model.train()\n",
    "\n",
    "            for batch in train_data_loader:\n",
    "                l_channel, a_channel, b_channel = batch\n",
    "                l_channel = l_channel.to(device)\n",
    "\n",
    "                a_b_channel = torch.cat([a_channel, b_channel], dim=1)\n",
    "                a_b_channel_hat = model(l_channel)\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    loss = lossF(a_b_channel_hat.float().cuda(),\n",
    "                                 a_b_channel.float().cuda()).to(device)\n",
    "                else:\n",
    "                    loss = lossF(a_b_channel_hat.float(),\n",
    "                                 a_b_channel.float()).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss_train += loss.item()\n",
    "\n",
    "            print(\"epoch: {0}, loss: {1}\"\n",
    "                  .format(epoch, total_loss_train))\n",
    "            loss_train.append(total_loss_train)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        plot_loss_epoch(loss_train, loss_plot_path)\n",
    "        torch.save(model.state_dict(), saved_model_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def validate(model, val_data_loader, lossF, device):\n",
    "        loss_valid = []\n",
    "        model.eval()\n",
    "\n",
    "        # val treated\n",
    "        for batch in val_data_loader:\n",
    "            l_channel, a_channel, b_channel = batch\n",
    "            l_channel = l_channel.to(device)\n",
    "\n",
    "            a_b_channel = torch.cat([a_channel, b_channel], dim=1)\n",
    "            a_b_channel_hat = model(l_channel)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                loss = lossF(a_b_channel_hat.float().cuda(),\n",
    "                             a_b_channel.float().cuda()).to(device)\n",
    "            else:\n",
    "                loss = lossF(a_b_channel_hat.float(),\n",
    "                             a_b_channel.float()).to(device)\n",
    "\n",
    "            loss_valid.append(loss.item())\n",
    "\n",
    "        valid_loss = np.average(loss_valid)\n",
    "        return valid_loss\n",
    "\n",
    "    def test(self, augmented_dataset_batch_train, activation_function, save_path,device):\n",
    "        print(activation_function)\n",
    "        data_loader = augmented_dataset_batch_train\n",
    "        saved_model_path =\"Model/Colorizer/Colorizer_{0}.pth\".format(activation_function)\n",
    "\n",
    "        epoch = 400\n",
    "        lr = .0001\n",
    "        weight_decay = 1e-5\n",
    "        in_channel = 3\n",
    "        hidden_channel = 3\n",
    "        loss_plot_path = \"Model/Colorizer/Colorizer_Loss_plot_{0}.jpeg\".format(activation_function)\n",
    "        \n",
    "\n",
    "        print(\"..Colorizer Test started..\")\n",
    "        model = Colorizer(in_channel=in_channel,\n",
    "                          hidden_channel=hidden_channel,\n",
    "                          activation_function=activation_function).to(device)\n",
    "        model.load_state_dict(torch.load(saved_model_path, map_location=device))\n",
    "\n",
    "        lossF = nn.MSELoss()\n",
    "        serial_num = 0\n",
    "        for batch in data_loader:\n",
    "            serial_num += 1\n",
    "            l_channel, a_channel, b_channel = batch\n",
    "            l_channel = l_channel.to(device)\n",
    "\n",
    "            a_b_channel = torch.cat([a_channel, b_channel], dim=1)\n",
    "            a_b_channel_hat = model(l_channel).detach()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                loss = lossF(a_b_channel_hat.float().cuda(),\n",
    "                             a_b_channel.float().cuda()).to(device)\n",
    "            else:\n",
    "                loss = lossF(a_b_channel_hat.float(),\n",
    "                             a_b_channel.float()).to(device)\n",
    "\n",
    "            print(\"Image: {0}, loss: {1}\".format(serial_num, loss.item()))\n",
    "\n",
    "            save_name_orig = 'Orig_img_epoch_{0}_lr_{1}_wt_decay{2}_serial_{3}_{4}.jpg' \\\n",
    "                .format(epoch, lr, weight_decay, serial_num,activation_function)\n",
    "            save_name_recons = 'Recons_img_epoch_{0}_lr_{1}_wt_decay{2}_serial_{3}_{4}.jpg' \\\n",
    "                .format(epoch, lr, weight_decay, serial_num,activation_function)\n",
    "\n",
    "            to_rgb(l_channel[0].cpu(), a_b_channel[0].cpu(),\n",
    "                         activation_function,\n",
    "                         save_path=save_path, save_name=save_name_orig, device=device)\n",
    "            to_rgb(l_channel[0].cpu(), a_b_channel_hat[0].cpu(),\n",
    "                         activation_function,\n",
    "                         save_path=save_path, save_name=save_name_recons, device=device)\n",
    "\n",
    "        self.show_final_image_grid(epoch, lr, weight_decay, save_path,activation_function)\n",
    "\n",
    "    @staticmethod\n",
    "    def show_final_image_grid(epoch, lr, weight_decay, save_path,activation_function):\n",
    "\n",
    "        color_path = save_path['colorized']\n",
    "        gray_path = save_path['grayscale']\n",
    "\n",
    "        for image_index in range(7, 70, 7):\n",
    "            title = \"Plots/Colorizer/epoch_{0}_lr_{1}_wt_{2}_serial_{3}_{4}.jpeg\".\\\n",
    "                format(epoch, lr, weight_decay, image_index,activation_function)\n",
    "\n",
    "            save_name_orig = 'Orig_img_epoch_{0}_lr_{1}_wt_decay{2}_serial_{3}_{4}.jpg' \\\n",
    "                .format(epoch, lr, weight_decay, image_index,activation_function)\n",
    "            save_name_recons = 'Recons_img_epoch_{0}_lr_{1}_wt_decay{2}_serial_{3}_{4}.jpg' \\\n",
    "                .format(epoch, lr, weight_decay, image_index,activation_function)\n",
    "            show_output_image(gray_path + save_name_orig, color_path + save_name_orig,\n",
    "                                    color_path + save_name_recons, title)\n",
    "            \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    data_load()\n",
    "\n",
    "    #print(\"For Sigmoid Function\")\n",
    "    #execution_colorizer_sigmoid()\n",
    "\n",
    "    print(\"For Tanh Function\")\n",
    "    execution_colorizer_tanh()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NGC-PyTorch-1.9",
   "language": "python",
   "name": "ngc-pytorch-1.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
