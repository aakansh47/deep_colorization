{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of given Image List 750\n",
      "Number of images -  751\n",
      "Training Set Size :  675\n",
      "Test Set Size :  76\n",
      "Length of training Image List 674\n",
      "Length of testing Image List 76\n",
      "For Sigmoid Function\n",
      "Device: cpu\n",
      "Length of Augmented Dataset 6740\n",
      "Activation Function:  sigmoid\n",
      "..Colorizer Training started..\n",
      "epoch: 0, loss: 1.672486498951912\n",
      "epoch: 1, loss: 0.545737168751657\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f69e3b7b9076>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"For Sigmoid Function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m     \u001b[0mexecution_colorizer_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"For Tanh Function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f69e3b7b9076>\u001b[0m in \u001b[0;36mexecution_colorizer_sigmoid\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mcolorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mManage_Colorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mcolorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_dataset_batch_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0mcolorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_dataset_batch_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mtrain_regressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_dataset_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f69e3b7b9076>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, augmented_dataset_batch_train, activation_function, device)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import os\n",
    "import re\n",
    "from shutil import copy2\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import lab2rgb\n",
    "from skimage.color import rgb2lab, rgb2gray\n",
    "from torchvision import datasets\n",
    "def data_load():\n",
    "    image_list = glob.glob('face_images/*.jpg')\n",
    "    print(\"Length of given Image List\", len(image_list))\n",
    "    train_test_split()\n",
    "    training_image_list = glob.glob('data/train/class/*.jpg')\n",
    "    test_image_list = glob.glob('data/test/class/*.jpg')\n",
    "    print(\"Length of training Image List\", len(training_image_list))\n",
    "    print(\"Length of testing Image List\", len(test_image_list))\n",
    "def train_test_split():\n",
    "        os.makedirs('data/train/class/', exist_ok=True)\n",
    "        os.makedirs('data/test/class/', exist_ok=True)\n",
    "        os.makedirs('Model/Colorizer/', exist_ok=True)\n",
    "        os.makedirs('Model/Regressor/', exist_ok=True)\n",
    "        os.makedirs('Plots/Colorizer/', exist_ok=True)\n",
    "        os.makedirs('outputs_sigmoid/gray/',exist_ok=True)\n",
    "        os.makedirs('outputs_sigmoid/color/',exist_ok=True)\n",
    "        os.makedirs('outputs_sigmoid/disp/',exist_ok=True)\n",
    "        os.makedirs('outputs_tanh/gray/',exist_ok=True)\n",
    "        os.makedirs('outputs_tanh/color/',exist_ok=True)\n",
    "\n",
    "\n",
    "        number_of_images = len(next(os.walk('face_images'))[2])\n",
    "        print(\"Number of images - \", number_of_images)\n",
    "\n",
    "        for i, file in enumerate(os.listdir('face_images')):\n",
    "            if i < (0.1 * number_of_images):\n",
    "                copy2('face_images/' + file, 'data/test/class/' + file)\n",
    "                continue\n",
    "            else: \n",
    "                copy2('face_images/' + file, 'data/train/class/' + file)\n",
    "\n",
    "        print(\"Training Set Size : \", len(next(os.walk('data/train/class'))[2]))\n",
    "        print(\"Test Set Size : \", len(next(os.walk('data/test/class'))[2]))\n",
    "def build_dataset(cuda=False, num_workers=1,\n",
    "                  activation_function=\"sigmoid\"):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(128),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(128)\n",
    "    ])\n",
    "\n",
    "    train_datasets = []\n",
    "    if activation_function == \"sigmoid\" or activation_function == 'tanh':\n",
    "        train_datasets.append(AugmentImageDataset('data/train'))\n",
    "    elif activation_function == \"relu\":\n",
    "        train_datasets.append(AugmentImageDataset_RELU('data/train'))\n",
    "\n",
    "    for i in range(9):\n",
    "        if activation_function == \"sigmoid\":\n",
    "            train_datasets.append(AugmentImageDataset('data/train', transform))\n",
    "        elif activation_function == \"relu\":\n",
    "            train_datasets.append(AugmentImageDataset_RELU('data/train', transform))\n",
    "        elif activation_function == \"tanh\":\n",
    "            train_datasets.append(AugmentImageDataset_Tanh('data/train',\n",
    "                                                                        transform))\n",
    "\n",
    "    augmented_dataset = ConcatDataset(train_datasets)\n",
    "    print(\"Length of Augmented Dataset\", len(augmented_dataset))\n",
    "\n",
    "    train_loader_args = dict(shuffle=True,\n",
    "                             batch_size=16,\n",
    "                             num_workers=num_workers, pin_memory=True) \\\n",
    "        if cuda else dict(shuffle=True, batch_size=32)\n",
    "\n",
    "    augmented_dataset_batch_train = DataLoader(dataset=augmented_dataset, **train_loader_args)\n",
    "    augmented_dataset_batch_test = DataLoader(dataset=AugmentImageDataset('data/test'))\n",
    "\n",
    "    return augmented_dataset_batch_train, augmented_dataset_batch_test\n",
    "\n",
    "def execution_colorizer_sigmoid():\n",
    "    activation_function = \"sigmoid\"\n",
    "    save_path = {'grayscale': 'outputs_sigmoid/gray/', 'colorized': 'outputs_sigmoid/color/'}\n",
    "    device, is_cuda_present, num_workers = get_device()\n",
    "    model_name = \"Model/Colorizer/Colorizer_sigmoid_epoch_{0}_lr_{1}_weight_decay_{2}.pth\"\n",
    "\n",
    "    print(\"Device: {0}\".format(device))\n",
    "    augmented_dataset_batch_train, \\\n",
    "    augmented_dataset_batch_test = build_dataset(is_cuda_present, num_workers,activation_function)\n",
    "\n",
    "    colorizer = Manage_Colorize()\n",
    "    colorizer.train(augmented_dataset_batch_train,activation_function, device)\n",
    "    colorizer.test(augmented_dataset_batch_test, activation_function,save_path, device)\n",
    "    train_regressor(augmented_dataset_batch_train, device)\n",
    "    test_regressor(augmented_dataset_batch_test, device)\n",
    "def execution_colorizer_tanh():\n",
    "    activation_function = \"tanh\"\n",
    "    save_path = {'grayscale': 'outputs_tanh/gray/', 'colorized': 'outputs_tanh/color/'}\n",
    "    device, is_cuda_present, num_workers = get_device()\n",
    "    model_name = \"Model/Colorizer/Colorizer_tanh_epoch_{0}_lr_{1}_weight_decay_{2}.pth\"\n",
    "\n",
    "    print(\"Device: {0}\".format(device))\n",
    "    augmented_dataset_batch_train, \\\n",
    "    augmented_dataset_batch_test = build_dataset(is_cuda_present, num_workers,activation_function)\n",
    "    colorizer = Manage_Colorize()\n",
    "    colorizer.train(augmented_dataset_batch_train,activation_function, device)\n",
    "    colorizer.test(augmented_dataset_batch_test, activation_function,save_path, device)\n",
    "    train_regressor(augmented_dataset_batch_train, device)\n",
    "    test_regressor(augmented_dataset_batch_test, device)\n",
    "    \n",
    "def train_regressor(augmented_dataset_batch_train, device):\n",
    "    data_loader = augmented_dataset_batch_train\n",
    "    saved_model_path = \"Model/Regressor/Regressor.pth\"\n",
    "    epochs = 2\n",
    "    lr = 0.0001\n",
    "    weight_decay = 1e-5\n",
    "    in_channel = 1\n",
    "    hidden_channel = 3\n",
    "    out_dims = 2\n",
    "    loss_plot_path = \"Model/Regressor/Regressor_Loss_plot.jpeg\"   \n",
    "\n",
    "    print(\"..Regressor training started..\")\n",
    "    model = Regressor(in_channel=in_channel,\n",
    "                        hidden_channel=hidden_channel,\n",
    "                        out_dims=out_dims,\n",
    "                        train_mode=\"regressor\").to(device)\n",
    "\n",
    "    lossF = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "    loss_train = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch in data_loader:\n",
    "            l_channel, a_channel, b_channel = batch\n",
    "            l_channel = l_channel.to(device)\n",
    "\n",
    "            a_b_mean = get_ab_mean(a_channel, b_channel)\n",
    "            a_b_mean_hat = model(l_channel)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                loss = lossF(a_b_mean_hat.float().cuda(),\n",
    "                                 a_b_mean.float().cuda()).to(device)\n",
    "            else:\n",
    "                loss = lossF(a_b_mean_hat.float(),\n",
    "                                a_b_mean.float()).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(\"epoch: {0}, loss: {1}\"\n",
    "                .format(epoch, total_loss))\n",
    "        loss_train.append(total_loss)\n",
    "\n",
    "    plot_loss_epoch(loss_train, loss_plot_path)\n",
    "    torch.save(model.state_dict(), saved_model_path)\n",
    "\n",
    "def test_regressor(augmented_dataset_batch_test, device):\n",
    "    data_loader = augmented_dataset_batch_test\n",
    "    saved_model_path = \"Model/Regressor/Regressor.pth\"\n",
    "    in_channel = 1\n",
    "    hidden_channel = 3\n",
    "    out_dims = 2\n",
    "\n",
    "    print(\"..Regressor testing started..\")\n",
    "\n",
    "    model = Regressor(in_channel=in_channel,\n",
    "                          hidden_channel=hidden_channel,\n",
    "                          out_dims=out_dims,\n",
    "                          train_mode=\"regressor\").to(device)\n",
    "    model.load_state_dict(torch.load(saved_model_path, map_location=device))\n",
    "\n",
    "    a_list = []\n",
    "    b_list = []\n",
    "    lossF = nn.MSELoss()\n",
    "    total_loss = 0\n",
    "    loss_train = []\n",
    "    for batch in data_loader:\n",
    "        l_channel, a_channel, b_channel = batch\n",
    "        l_channel = l_channel.to(device)\n",
    "\n",
    "        a_b_mean = get_ab_mean(a_channel, b_channel)\n",
    "        a_b_mean_hat = model(l_channel).detach()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            loss = lossF(a_b_mean_hat.float().cuda(),\n",
    "                             a_b_mean.float().cuda()).to(device)\n",
    "        else:\n",
    "            loss = lossF(a_b_mean_hat.float(),\n",
    "                             a_b_mean.float()).to(device)\n",
    "\n",
    "        loss_train.append(loss.item())\n",
    "\n",
    "        a_b_pred = a_b_mean_hat[0].cpu().numpy()\n",
    "        a_list.append(a_b_pred[0])\n",
    "        b_list.append(a_b_pred[1])\n",
    "\n",
    "    print(\"MSE:\", np.average(np.asarray(loss_train)))\n",
    "    print(\"Image_num || Mean a || Mean b\")\n",
    "    for i in range(1, len(a_list)):\n",
    "        print(\"Image: {0} mean_a: {1} mean_b:{2}\".format(\n",
    "            i, (a_list[i] * 255) - 128, (b_list[i] * 255) - 128\n",
    "        ))\n",
    "    \n",
    "def get_device():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    is_cuda_present = True if torch.cuda.is_available() else False\n",
    "    num_workers = 8 if is_cuda_present else 0\n",
    "    return device, is_cuda_present, num_workers\n",
    "class AugmentImageDataset(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        global img_a, img_b, img_gray\n",
    "        path, target = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        img_lab = rgb2lab(np.asarray(img))\n",
    "        img_lab = img_lab + 128\n",
    "        img_lab = img_lab / 255\n",
    "        img_a = torch.from_numpy(img_lab[:, :, 1:2].transpose((2, 0, 1))).float() \n",
    "        img_b = torch.from_numpy(img_lab[:, :, 2:3].transpose((2, 0, 1))).float()\n",
    "        img_gray = torch.from_numpy(rgb2gray(np.asarray(img))).unsqueeze(0).float()\n",
    "        return img_gray, img_a, img_b\n",
    "\n",
    "class AugmentImageDataset_Tanh(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        global img_a, img_b, img_gray\n",
    "        path, target = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        img_lab = rgb2lab(np.asarray(img))\n",
    "        img_lab = img_lab + 128\n",
    "        img_lab = img_lab / 255\n",
    "        img_a = torch.from_numpy(img_lab[:, :, 1:2].transpose((2, 0, 1))).float()\n",
    "        img_b = torch.from_numpy(img_lab[:, :, 2:3].transpose((2, 0, 1))).float()\n",
    "        img_gray = torch.from_numpy(rgb2gray(np.asarray(img))).unsqueeze(0).float()\n",
    "        return img_gray, img_a, img_b\n",
    "\n",
    "class AugmentImageDataset_RELU(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        global img_a, img_b, img_gray\n",
    "        path, target = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        img_lab = rgb2lab(np.asarray(img))\n",
    "        img_a = torch.from_numpy(img_lab[:, :, 1:2].transpose((2, 0, 1))).float() \n",
    "        img_b = torch.from_numpy(img_lab[:, :, 2:3].transpose((2, 0, 1))).float()\n",
    "        img_gray = torch.from_numpy(rgb2gray(np.asarray(img))).unsqueeze(0).float()\n",
    "\n",
    "        return img_gray, img_a, img_b\n",
    "def get_ab_mean(a_channel, b_channel):\n",
    "    a_channel_mean = a_channel.mean(dim=(2, 3))\n",
    "    b_channel_mean = b_channel.mean(dim=(2, 3))\n",
    "    a_b_mean = torch.cat([a_channel_mean,\n",
    "                              b_channel_mean], dim=1)\n",
    "    return a_b_mean\n",
    "def plot_loss_epoch(train_loss_avg, fig_name):\n",
    "    plt.ion()\n",
    "    fig = plt.figure()\n",
    "    plt.plot(train_loss_avg)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.draw()\n",
    "    plt.savefig(fig_name, dpi=220)\n",
    "    plt.clf()\n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, in_channel=1, hidden_channel=3, out_dims=2,\n",
    "                 train_mode=\"regressor\"):\n",
    "        super(Regressor, self).__init__()\n",
    "        self.train_mode = train_mode\n",
    "\n",
    "        self.feature_maps = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channel, out_channels=32,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=256,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=512,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        if self.train_mode == \"regressor\":\n",
    "            self.lin = nn.Linear(in_features=512 * 2 * 2, out_features=out_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_maps = self.feature_maps(x)\n",
    "        if self.train_mode == \"regressor\":\n",
    "            y_hat = torch.sigmoid(self.lin(feature_maps.reshape(-1, 512 * 2 * 2)))\n",
    "            return y_hat\n",
    "\n",
    "        else:\n",
    "            return feature_maps\n",
    "\n",
    "class Colorizer(nn.Module):\n",
    "    def __init__(self, in_channel=3, hidden_channel=3, out_channel=2,\n",
    "                 activation_function=\"sigmoid\"):\n",
    "        super(Colorizer, self).__init__()\n",
    "        self.activation_function = activation_function\n",
    "        self.feature_maps = Regressor(in_channel=1, hidden_channel=3, out_dims=2,\n",
    "                                      train_mode=\"colorizer\")\n",
    "        self.up_sample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=256,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=256,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=32,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=out_channel,\n",
    "                               kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            return torch.sigmoid(self.up_sample(self.feature_maps(x)))\n",
    "        elif self.activation_function == \"tanh\":\n",
    "            return torch.tanh(self.up_sample(self.feature_maps(x)))\n",
    "        elif self.activation_function == \"relu\":\n",
    "            return torch.relu(self.up_sample(self.feature_maps(x)))\n",
    "\n",
    "def show_img(image):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    np_img = image.numpy()\n",
    "    plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "    plt.show()\n",
    "def to_rgb(grayscale_input, ab_input, activation_function=\"tanh\",\n",
    "               save_path=None, save_name=None, device=\"cpu\"):\n",
    "    plt.clf()\n",
    "    color_image = torch.cat((grayscale_input, ab_input), 0).numpy()  # combine channels\n",
    "    color_image = color_image.transpose((1, 2, 0))  # rescale for matplotlib\n",
    "    color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n",
    "    color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128\n",
    "    color_image = lab2rgb(color_image.astype(np.float64))\n",
    "    grayscale_input = grayscale_input.squeeze().numpy()\n",
    "    if save_path is not None and save_name is not None:\n",
    "        plt.imsave(arr=grayscale_input, fname='{}{}'.format(save_path['grayscale'], save_name), cmap='gray')\n",
    "        plt.imsave(arr=color_image, fname='{}{}'.format(save_path['colorized'], save_name))\n",
    "def show_img_tensor(image):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(image.permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "def show_output_image(gray, orig, recons, fig_name):\n",
    "    plt.clf()\n",
    "    f = plt.figure()\n",
    "    f.add_subplot(1, 3, 1)\n",
    "    plt.imshow(mpimg.imread(gray))\n",
    "    plt.axis('off')\n",
    "    f.add_subplot(1, 3, 2)\n",
    "    plt.imshow(mpimg.imread(orig))\n",
    "    plt.axis('off')\n",
    "    f.add_subplot(1, 3, 3)\n",
    "    plt.imshow(mpimg.imread(recons))\n",
    "    plt.axis('off')\n",
    "    plt.draw()\n",
    "    plt.savefig(fig_name, dpi=220)\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    \n",
    "class EarlyStopping_DCN:\n",
    "\n",
    "    def __init__(self, patience=7, verbose=False, delta=0,\n",
    "                 model_path=None,\n",
    "                 trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.model_path = model_path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            self.trace_func(\n",
    "                f'Validation loss decreased ({self.val_loss_min} --> {val_loss}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.model_path)\n",
    "        self.val_loss_min = val_loss\n",
    "        \n",
    "class Manage_Colorize:\n",
    "    def train(self,augmented_dataset_batch_train, activation_function, device):\n",
    "        print(\"Activation Function: \", activation_function)\n",
    "\n",
    "        train_data_loader = augmented_dataset_batch_train\n",
    "        saved_model_path =\"Model/Colorizer/Colorizer_{0}.pth\".format(activation_function)\n",
    "\n",
    "        epochs = 2\n",
    "        lr = .0001\n",
    "        weight_decay = 1e-5\n",
    "        in_channel = 3\n",
    "        hidden_channel = 3\n",
    "        loss_plot_path = \"Model/Colorizer/Colorizer_Loss_plot_{0}.jpeg\".format(activation_function)\n",
    "\n",
    "        print(\"..Colorizer Training started..\")\n",
    "        model = Colorizer(in_channel=3, hidden_channel=3,\n",
    "                          activation_function=activation_function).to(device)\n",
    "\n",
    "        lossF = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr,\n",
    "                               weight_decay=weight_decay)\n",
    "        loss_train = []\n",
    "        early_stopping = EarlyStopping_DCN(patience=50, verbose=True,\n",
    "                                           model_path=saved_model_path)\n",
    "        for epoch in range(epochs):\n",
    "            total_loss_train = 0\n",
    "            total_loss_val = 0\n",
    "            model.train()\n",
    "\n",
    "            for batch in train_data_loader:\n",
    "                l_channel, a_channel, b_channel = batch\n",
    "                l_channel = l_channel.to(device)\n",
    "\n",
    "                a_b_channel = torch.cat([a_channel, b_channel], dim=1)\n",
    "                a_b_channel_hat = model(l_channel)\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    loss = lossF(a_b_channel_hat.float().cuda(),\n",
    "                                 a_b_channel.float().cuda()).to(device)\n",
    "                else:\n",
    "                    loss = lossF(a_b_channel_hat.float(),\n",
    "                                 a_b_channel.float()).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss_train += loss.item()\n",
    "\n",
    "            print(\"epoch: {0}, loss: {1}\"\n",
    "                  .format(epoch, total_loss_train))\n",
    "            loss_train.append(total_loss_train)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        plot_loss_epoch(loss_train, loss_plot_path)\n",
    "        torch.save(model.state_dict(), saved_model_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def validate(model, val_data_loader, lossF, device):\n",
    "        loss_valid = []\n",
    "        model.eval()\n",
    "\n",
    "        # val treated\n",
    "        for batch in val_data_loader:\n",
    "            l_channel, a_channel, b_channel = batch\n",
    "            l_channel = l_channel.to(device)\n",
    "\n",
    "            a_b_channel = torch.cat([a_channel, b_channel], dim=1)\n",
    "            a_b_channel_hat = model(l_channel)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                loss = lossF(a_b_channel_hat.float().cuda(),\n",
    "                             a_b_channel.float().cuda()).to(device)\n",
    "            else:\n",
    "                loss = lossF(a_b_channel_hat.float(),\n",
    "                             a_b_channel.float()).to(device)\n",
    "\n",
    "            loss_valid.append(loss.item())\n",
    "\n",
    "        valid_loss = np.average(loss_valid)\n",
    "        return valid_loss\n",
    "\n",
    "    def test(self, augmented_dataset_batch_train, activation_function, save_path,device):\n",
    "        print(activation_function)\n",
    "        data_loader = augmented_dataset_batch_train\n",
    "        saved_model_path =\"Model/Colorizer/Colorizer_{0}.pth\".format(activation_function)\n",
    "\n",
    "        epoch = 2\n",
    "        lr = .0001\n",
    "        weight_decay = 1e-5\n",
    "        in_channel = 3\n",
    "        hidden_channel = 3\n",
    "        loss_plot_path = \"Model/Colorizer/Colorizer_Loss_plot_{0}.jpeg\".format(activation_function)\n",
    "        \n",
    "\n",
    "        print(\"..Colorizer Test started..\")\n",
    "        model = Colorizer(in_channel=in_channel,\n",
    "                          hidden_channel=hidden_channel,\n",
    "                          activation_function=activation_function).to(device)\n",
    "        model.load_state_dict(torch.load(saved_model_path, map_location=device))\n",
    "\n",
    "        lossF = nn.MSELoss()\n",
    "        serial_num = 0\n",
    "        for batch in data_loader:\n",
    "            serial_num += 1\n",
    "            l_channel, a_channel, b_channel = batch\n",
    "            l_channel = l_channel.to(device)\n",
    "\n",
    "            ab_channel = torch.cat([a_channel, b_channel], dim=1)\n",
    "            a_b_channel_hat = model(l_channel).detach()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                loss = lossF(a_b_channel_hat.float().cuda(),\n",
    "                             a_b_channel.float().cuda()).to(device)\n",
    "            else:\n",
    "                loss = lossF(a_b_channel_hat.float(),\n",
    "                             a_b_channel.float()).to(device)\n",
    "\n",
    "            print(\"Image: {0}, loss: {1}\".format(serial_num, loss.item()))\n",
    "\n",
    "            save_name_orig = 'Orig_img_epoch_{0}_lr_{1}_wt_decay{2}_serial_{3}_{4}.jpg' \\\n",
    "                .format(epoch, lr, weight_decay, serial_num,activation_function)\n",
    "            save_name_recons = 'Recons_img_epoch_{0}_lr_{1}_wt_decay{2}_serial_{3}_{4}.jpg' \\\n",
    "                .format(epoch, lr, weight_decay, serial_num,activation_function)\n",
    "\n",
    "            to_rgb(l_channel[0].cpu(), a_b_channel[0].cpu(),\n",
    "                         activation_function,\n",
    "                         save_path=save_path, save_name=save_name_orig, device=device)\n",
    "            to_rgb(l_channel[0].cpu(), a_b_channel_hat[0].cpu(),\n",
    "                         activation_function,\n",
    "                         save_path=save_path, save_name=save_name_recons, device=device)\n",
    "\n",
    "        self.show_final_image_grid(epoch, lr, weight_decay, save_path,activation_function)\n",
    "\n",
    "    @staticmethod\n",
    "    def show_final_image_grid(epoch, lr, weight_decay, save_path,activation_function):\n",
    "\n",
    "        color_path = save_path['colorized']\n",
    "        gray_path = save_path['grayscale']\n",
    "\n",
    "        for image_index in range(7, 70, 7):\n",
    "            title = \"Plots/Colorizer/epoch_{0}_lr_{1}_wt_{2}_serial_{3}_{4}.jpeg\".\\\n",
    "                format(epoch, lr, weight_decay, image_index,activation_function)\n",
    "\n",
    "            save_name_orig = 'Orig_img_epoch_{0}_lr_{1}_wt_decay{2}_serial_{3}_{4}.jpg' \\\n",
    "                .format(epoch, lr, weight_decay, image_index,activation_function)\n",
    "            save_name_recons = 'Recons_img_epoch_{0}_lr_{1}_wt_decay{2}_serial_{3}_{4}.jpg' \\\n",
    "                .format(epoch, lr, weight_decay, image_index,activation_function)\n",
    "            show_output_image(gray_path + save_name_orig, color_path + save_name_orig,\n",
    "                                    color_path + save_name_recons, title)\n",
    "            \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    data_load()\n",
    "\n",
    "    print(\"For Sigmoid Function\")\n",
    "    execution_colorizer_sigmoid()\n",
    "\n",
    "    print(\"For Tanh Function\")\n",
    "    execution_colorizer_tanh()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NGC-PyTorch-1.9",
   "language": "python",
   "name": "ngc-pytorch-1.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
